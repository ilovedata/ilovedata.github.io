[
["index.html", "회귀모형 이론과 계산 서론", " 회귀모형 이론과 계산 이용희 2020년 9월 10일 서론 이 책은 가장 기초적인 선형회귀모형부터 일반화 선형모형, 혼합모형과 같은 복잡한 회귀모형 대한 이론을 가능도 함수 위주로 설명합니다. 또한 모형을 적합하는 계산법과 연관된 행렬이론에 대하여 다루고자 합니다. 이 책에서 사용된 기호와 표기법은 다음과 같습니다. 스칼라(scalar)와 확률변수는 모두 소문자 또는 대문자인 보통 글씨체로 표기한다. 다변량 확률벡터와 행렬은 굵은 글자체로 표기한다. library(DT) library(ggplot2) library(xfun) library(JuliaCall) # 아래 3 문장은 한글을 포함한 ggplot 그림이 포함된 HTML, PDF로 만드는 경우 사용 library(showtext) font_add_google(&quot;Nanum Pen Script&quot;, &quot;gl&quot;) showtext_auto() 이 책에서 사용된 기호와 표기법은 다음과 같습니다. 스칼라(scalar)와 확률변수는 모두 소문자 또는 대문자인 보통 글씨체로 표기한다. 다변량 확률벡터와 행렬은 굵은 글자체로 표기한다. 참고문헌은 Xie (2015) 이렇게 References "],
["chapintro.html", "제 1 장 회귀모형에서의 기초 추정법 1.1 단순선형회귀분석 1.2 회귀식의 행렬형식 1.3 최소제곱추정", " 제 1 장 회귀모형에서의 기초 추정법 1.1 단순선형회귀분석 1.1.1 예제: 자동차의 속도와 제동거리 자동차가 달리는 속도(speed,mph)와 제동거리(dist,ft)의 관계를 알아보기 위하여 50대의 자동차로 실험한 결과 자료와 산포도가 아래와 같다. ggplot(cars, aes(x=speed, y=dist)) + geom_point() + labs(x = &quot;속도&quot;, y = &quot;거리&quot;) 위와 같은 자료를 이용하여 자동차의 속도가 주어졌을 경우 제동거리의 평균에 대한 예측을 하려고 한다면 어떤 방법을 사용해야 할까? 회귀분석(regression analysis)는 여러 가지 변수들의 관계를 분석하는 통계적 방법이다. 일반적으로 한 개 또는 여러 가지의 설명변수들(explanatory variables)이 관심있는 종속변수(response variable)에 어떤 형태로 영향을 미치는지에 파악하고 설명변수와 종속변수의 관계를 통계적으로 추론하는 것이 회귀분석의 목적이다. 자동차의 속도를 \\(x\\) 라고 하고 제동거리를 \\(y\\) 라고 하면 다음과 같은 선형식으로 자동차의 속도와 제동거리의 관계를 나타내는 것을 단순선형회귀식(simple linear regression equation)이라고 한다. \\[\\begin{equation} E(y|x) = \\beta_0 + \\beta_1 x \\tag{1.1} \\end{equation}\\] 식 (1.1)은 \\(y\\)의 평균이 \\(x\\) 의 선형식으로 나타나는 관계를 가정한 것이며 절편 \\(\\beta_0\\) 와 기울기 \\(\\beta_1\\)은 모르는 모수로서 자료를 통하여 추정해야 한다. 위에서 본 cars 예제와 같이 \\(n\\) 개의 자료를 독립적으로 추출하였다면 자료의 생성 과정을 다음과 같은 선형회귀모형(linear regression model)로 나타낸다. 종속변수 \\(y\\)는 설명변수 \\(x\\)의 선형식으로 나타내어지는 결정적인 요인과 확률 변수로 나타내어지는 임의의 오차항 \\(e\\)의 합으로 나타내어진다. \\[\\begin{equation} y_i = E(y_i | x_i) + e_i = \\beta_0 + \\beta_1 x_i + e_i, \\quad i=1,2,\\dots,n \\tag{1.2} \\end{equation}\\] 위에서 오차항 \\(e_i\\) 평균이 \\(0\\)이고 분산이 \\(\\sigma^2\\) 인 임의의 확률분포를 따르며 서로 독립이다. \\[ E(e_i)=0, \\quad V(e_i) = \\sigma^2 \\quad i=1,2,\\dots,n \\] 1.1.2 최소제곱법 단순회귀식 (1.2)에서 모수 \\(\\beta_0\\)와 \\(\\beta_1\\)를 회귀계수(regression coefficient) 라고 하며 자료(observation; data)를 수집하여 추정해야 한다. \\(n\\)개의 자료를 이용하여 회귀계수 \\(\\beta_0\\)와 \\(\\beta_1\\)를 추정하려고 할 때 가장 쉽고 오래됬으며 또한 가장 유용한 방법인 최소제곱법(least square method)을 사용할 수 있다. 일단 위의 식 (1.2)에서 종속변수의 관측값 \\(y_i\\)을 대응하는 설명변수 \\(x=x_i\\)를 이용하여 예측한 값은 \\(\\beta_0 + \\beta_1 x_i\\)이다. 여기서 실제 관측하여 얻어진 값 \\(y_i\\)와 예측값 \\(\\beta_0 + \\beta_1 x_i\\)사이에는 차이가 발생한다. 그 차이를 잔차(residual)라고 하며 표현하면 다음과 같다. \\[ r_i = y_i - E(y_i|x_i) = y_i - ( \\beta_0 + \\beta_1 x_i) \\] 잔차는 위에 식에서 알 수 있듯이 관측값과 회귀식을 통한 예측값의 차이를 나타낸 것이다. 그러면 자료를 가장 잘 설명할 수 있는 회귀직선을 얻기 위해서는 잔차 \\(r_i\\)를 가장 작게하는 회귀모형을 세워야 한다. 잔차들을 최소로 하는 방법들 중 하나인 최소제곱법은 잔차들의 제곱합을 최소로 하는 회귀계수를 추정하는 방법이다. 잔차들의 제곱합은 다음과 같이 표현된다. \\[\\begin{equation} S(\\beta_0 , \\beta_1) = \\sum^n_{i=1}r^2_i = \\sum^n_{i=1}[y_i-(\\beta_0 + \\beta_1 x_i)]^2 \\tag{1.3} \\end{equation}\\] 위의 잔차 제곱합 \\(S(\\beta_0 , \\beta_1)\\) 을 최소화하는 \\(\\beta_0\\)와 \\(\\beta_1\\)의 값을 구하는 방법은 잔차 제곱합이 \\(\\beta_0\\)와 \\(\\beta_1\\)의 미분 가능한 2차 함수이고 아래로 볼록한 함수(convex function)임을 이용한다. 각각의 회귀계수에 대해서 편미분을 하고 0으로 놓으면 아래와 같이 정리된다. \\[\\begin{align} \\pardiff{ S(\\beta_0 , \\beta_1)}{\\beta_0} &amp; = \\sum^n_{i=1}(-2)[y_i-(\\beta_0+\\beta_1 x_i)]=0 \\tag{1.4} \\\\ \\pardiff{ S(\\beta_0 , \\beta_1)}{\\beta_1} &amp; = \\sum^n_{i=1}(-2 x_i)[y_i-(\\beta_0+\\beta_1 x_i)]=0 \\tag{1.5} \\end{align}\\] 위의 연립방정식을 행렬식으로 표시하면 다음과 같이 나타낼 수 있다. \\[ \\begin{bmatrix} n &amp; \\sum_i x_i \\\\ \\sum_i x_i &amp; \\sum_i x^2_i \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\end{bmatrix} = \\begin{bmatrix} \\sum_i y_i \\\\ \\sum_i x_i y_i \\end{bmatrix} \\] 위의 방정식을 풀어서 구한 회귀계수의 추정치를 \\(\\hat \\beta_0\\), \\(\\hat \\beta_1\\) 이라고 하면 다음과 같이 주어진다. \\[\\begin{align*} \\hat \\beta_0 &amp;= \\bar y - \\hat \\beta_1 \\bar x \\\\ \\hat \\beta_1 &amp;= \\frac{ \\sum_i (x_i - \\bar x)(y_i - \\bar y)}{\\sum_i (x_i - \\bar x)^2} \\end{align*}\\] 1.2 회귀식의 행렬형식 일반적으로 회귀모형에서 종속변수의 수는 하나인 경우가 많지만 설명변수의 수는 여러 개인 경우가 많다. 이런 경우 중선형회귀식(multiple linear regression)은 다음과 같이 표현할 수 있고, \\(p\\)개의 설명변수가 있다고 가정하고 \\((x_1, x_2, \\cdots, x_p)\\) 표본의 크기 \\(n\\)인 자료가 얻어지면 선형회귀식을 행렬로 다음과 같이 표현할 수 있다. \\[\\begin{align} y_i &amp; = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} + e_i \\tag{1.6} \\\\ &amp; = \\bm x^t_i \\bm \\beta + e_i \\tag{1.7} \\end{align}\\] 위의 식을 다시 표현하면 다음과 같이 쓸 수 있다. \\(n\\)개의 관측치가 있을때 n개의 회귀식을 행렬식으로 표현하면 다음과 같다. 위의 식을 벡터를 이용하여 표시하면 다음과 같다. \\[\\begin{equation} \\bm y = \\bm X \\bm \\beta + \\bm e \\tag{1.8} \\end{equation}\\] 위의 행렬식에서 각 벡터와 행렬의 차원은 다음과 같다. \\(\\bm y\\) \\(n \\times 1\\) \\(\\bm X\\) \\(n \\times (p+1)\\) \\(\\bm \\beta\\) \\((p+1) \\times 1\\) \\(\\bm e\\) \\(n \\times 1\\) 여기서 회귀분석의 오차항은 서로 독립이고 동일한 분산을 갖는다. 즉, 오차항은 다음의 분포를 따른다. 즉, \\(E(\\bm e) = \\bm 0\\) 이므로 관측값 벡터 \\(\\bm y\\)의 평균을 보면 \\[\\begin{equation} E(\\bm y|\\bm X) = E(\\bm X \\bm \\beta+\\bm e)= \\bm X \\bm \\beta + E(\\bm e) = \\bm X\\bm \\beta \\tag{1.9} \\end{equation}\\] 1.3 최소제곱추정 최소제곱추정법(least square estimation)은 자료의 관계을 잘 반영하는 회귀식을 구한 다음 실제 관측값 \\(y_i\\)과 예측값 \\(\\bm x_i^t \\bm \\beta\\) 간에 차이인 잔차를 가장 작게 만드는 것이 목적이다. 모든 잔차항의 제곱의 합을 최소화하는 방법을 최소제곱법이라고 하며 이를 이용하여 회귀계수의 추정량을 찾는다. \\[\\begin{equation} \\min_{\\bm \\beta} \\sum_{i=1}^n (y_i - \\bm x_i^t \\bm \\beta )^2 = \\min_{\\bm \\beta } ( \\bm y - \\bm X \\bm \\beta )^t( \\bm y - \\bm X \\bm \\beta ) \\tag{1.10} \\end{equation}\\] 1.3.1 방법 1 \\(\\hat {\\bm \\beta}\\)는 잔차의 제곱합 (1.10) 을 최소로 하는 최소제곱 추정량이다. 잔차의 제곱합을 \\(S( \\bm \\beta)\\)이라고 하면 \\[\\begin{align} S( \\bm \\beta ) &amp; = ( \\bm y - \\bm X \\bm \\beta)^t( \\bm y - \\bm X \\bm \\beta ) \\notag \\\\ &amp; = \\bm y^t \\bm y - \\bm y^t \\bm X \\bm \\beta - \\bm \\beta^t \\bm X^t \\bm y + \\bm \\beta^t \\bm X^t \\bm X \\bm \\beta \\notag \\\\ &amp; = \\bm y^t \\bm y -2 \\bm \\beta^t \\bm X^t \\bm y + \\bm \\beta^t \\bm X^t \\bm X \\bm \\beta \\tag{1.11} \\end{align}\\] 여기서 \\(S( \\bm \\beta)\\)를 최소로 하는 회귀계수벡터의 값을 구하기 위하여 \\(S( \\bm \\beta)\\)를 회귀계수벡터 \\(\\bm \\beta\\)로 미분한후 \\(\\bm 0\\) 으로 놓고 선형 방정식을 풀어야 한다. 앞 절에 나오는 벡터미분을 이용하면 \\[\\begin{align*} \\pardiff{ S( {\\bm \\beta})}{\\bm \\beta} &amp; = \\pardiff{}{\\bm \\beta} (\\bm y^t \\bm y -2 \\bm \\beta^t \\bm X^t \\bm y + \\bm \\beta^t \\bm X^t \\bm X \\bm \\beta) \\\\ &amp; = \\bm 0 -2 \\bm X^t \\bm y + 2 \\bm X^t \\bm X \\bm \\beta \\\\ &amp; =\\bm 0 \\end{align*}\\] 최소제곱 추정량을 구하기 위한 정규방정식은 다음과 같이 쓸 수 있다. \\[\\begin{equation} \\bm X^t \\bm X \\bm \\beta = \\bm X^t \\bm y \\tag{1.12} \\end{equation}\\] 방정식 (1.12)를 정규방정식(normal equation)이라고 한다. 만약 \\(\\bm X^t \\bm X\\)가 정칙행렬일 경우 최소제곱법에 의한 회귀계수 추정량 \\(\\hat {\\bm \\beta}\\) 다음과 같다. \\[\\begin{equation} \\hat {\\bm \\beta} = ( \\bm X^t \\bm X)^{-1} \\bm X^t \\bm y \\tag{1.13} \\end{equation}\\] 예측값 벡터 \\(\\hat {\\bm y}\\) 는 \\(E(\\bm y | \\bm X)\\)의 추정치로서 다음과 같다. \\[ \\hat E(\\bm y | \\bm X)= \\hat {\\bm y} = \\bm X \\hat {\\bm \\beta} = \\bm X(\\bm X^t \\bm X)^{-1} \\bm X^t y \\] 만약 \\(\\bm X^t \\bm X\\)가 정칙행렬이 아닐 경우 최소제곱법에 의한 회귀계수 추정량 \\(\\hat {\\bm \\beta}\\)은 \\(\\bm X^t \\bm X\\)의 일반화 역행렬 \\((\\bm X^t \\bm X)^-\\)를 이용하여 다음과 같이 구한다. 이 경우 일반화 역행렬이 유일하지 않기 때문에 회귀계수 추정량도 유일하지 않다. \\[ \\hat {\\bm \\beta} = ( \\bm X^t \\bm X)^{-} \\bm X^t \\bm y \\] 1.3.2 방법 2 식 (1.10)에서 나오는 오차벡터를 정의하고 \\(\\bm e = (\\bm y - \\bm X \\bm \\beta)\\) 오차벡터를 모수벡터 \\(\\bm \\beta\\)로 미분하면 다음과 같은 결과를 얻는다. \\[ \\pardiff{\\bm e}{\\bm \\beta} = \\pardiff{ (\\bm y - \\bm X \\bm \\beta)}{ \\bm \\beta} = - \\pardiff{ \\bm X \\bm \\beta}{ \\bm \\beta} \\equiv - \\pardiff{\\bm \\beta^t \\bm X^t }{\\bm \\beta} = -\\bm X^t \\] 이제 오차제곱합 \\(S( {\\bm \\beta})=\\bm e^t \\bm e\\) 를 모수벡터로 미분하면 이차형식의 미분공식과 합성함수 미분공식을 차례로 적용하면 된다. \\[ \\pardiff{ S( {\\bm \\beta})}{\\bm \\beta}=\\pardiff{\\bm e^t \\bm e}{\\bm \\beta} = \\pardiff{\\bm e }{\\bm \\beta} \\pardiff{\\bm e^t \\bm e}{\\bm e} = -\\bm X^t \\left( 2 \\bm e \\right ) = -2 \\bm X^t (\\bm y - \\bm X \\bm \\beta) \\] 위의 방정식을 \\(\\bm 0\\)으로 놓으면 최소제곱 추정량 (열)벡터를 구한다. \\[ \\bm X^t \\bm y - \\bm X^t \\bm X \\bm \\beta = \\bm 0 \\quad \\rightarrow \\quad \\hat{\\bm \\beta} = (\\bm X^t \\bm X)^{-1} \\bm X^t \\bm y \\] "],
["chaplike.html", "제 2 장 가능도함수 2.1 가능도함수 개요 2.2 가능도함수와 그 성질 2.3 독립표본 2.4 지수군 분포 2.5 선형모형 2.6 최대가능도 추정량의 점근적 성질 (update 필요)", " 제 2 장 가능도함수 2.1 가능도함수 개요 2.2 가능도함수와 그 성질 확률변수 \\(y\\) 가 확률밀도함수 \\(f(y;\\bm \\theta)\\)를 따른다고 하자. 모수벡터 \\(\\bm \\theta\\) 에 대한 가능도함수 \\(L(\\bm \\theta;y)\\)와 로그가능도함수 \\(\\ell(\\bm \\theta;y)\\)는 다음과 같이 정의한다. \\[ L(\\bm \\theta) = L(\\bm \\theta;y) \\equiv f(y;\\bm \\theta), \\quad \\ell(\\bm \\theta) = \\ell(\\bm \\theta;y) \\equiv \\log L(\\bm \\theta;y) \\] 로그가능도함수를 모수 \\(\\bm \\theta\\)로 한 번 미분한 도함수(greadient)를 스코어함수(score function) \\(\\bm s(\\bm \\theta;y)\\) 로 아래와 같이 정의한다. 또한 두 번 미분한 헤시안(hessian)의 음수를 관측피셔정보(observed Fisher information) \\(J(\\bm \\theta;y)\\) 라고 정의한다. \\[ \\bm s(\\bm \\theta) = \\bm s(\\bm \\theta;y) \\equiv \\frac{\\partial}{\\partial \\bm \\theta } \\ell ( \\bm \\theta;y) ,\\quad \\bm J(\\bm \\theta) = \\bm J(\\bm \\theta;y) \\equiv -\\frac{\\partial^2}{\\partial {\\bm \\theta}^2 } \\ell ( \\bm \\theta;y) \\] 위의 식에서 만약 모수벡터 \\(\\bm \\theta\\)의 차원이 \\(p\\)라면 \\(\\bm s(\\bm \\theta)\\)는 \\(p \\times 1\\) 벡터이고 \\(\\bm J(\\bm \\theta;y)\\)는 \\(p \\times p\\)행렬이다. 로그가능도함수는 다음의 두 가지 중요한 방정식을 만족한다. \\[\\begin{equation} E \\left \\{ \\frac{\\partial}{\\partial \\bm \\theta } \\ell ( \\bm \\theta;y) \\right \\} =0 \\tag{2.1} \\end{equation}\\] \\[\\begin{equation} E \\left \\{ \\left [ \\frac{\\partial}{\\partial \\bm \\theta } \\ell ( \\bm \\theta;y) \\right ] \\left [ \\frac{\\partial}{\\partial \\bm \\theta} \\ell ( \\bm \\theta;y) \\right ]^t \\right\\} + E \\left \\{ \\frac{\\partial^2}{\\partial {\\bm \\theta}^2 } \\ell ( \\bm \\theta;y) \\right \\}= 0 \\tag{2.2} \\end{equation}\\] 식 (2.1)와 식 (2.2) 으로부터 다음과 같은 식이 유도되며 \\[ E [\\bm s(\\bm \\theta;y)] =0, \\quad E [\\bm s(\\bm \\theta;y)s^t(\\bm \\theta;y)] = E[\\bm J(\\bm \\theta;y)] \\] 다음과 같은 공식이 주어진다. \\[\\begin{align*} Var[\\bm s(\\bm \\theta;y)] &amp; = E[\\bm s(\\bm \\theta;y){\\bm s}^t(\\bm \\theta;y)] - \\{ E[\\bm s(\\bm \\theta;y)]E[ {\\bm s}^t(\\bm \\theta;y)] \\} \\\\ &amp; = E[\\bm s(\\bm \\theta;y){\\bm s}^t(\\bm \\theta;y)] - \\bm 0 \\\\ &amp; = -E \\left [ \\frac{\\partial^2}{\\partial {\\bm \\theta}^2 } \\log f(y;\\bm \\theta) \\right ] \\\\ &amp; = E[\\bm J(\\bm \\theta;y)] \\\\ &amp; \\equiv \\bm I(\\bm \\theta) \\end{align*}\\] 위의 식에서 스코어함수의 분산을 피셔정보(Fisher information)이라고 부르며 \\(\\bm I(\\bm \\theta)\\)로 표기한다. 관측피셔정보(observed Fisher information) \\(J(\\bm \\theta;y)\\) 는 모수와 확률변수로 정의되는 확률 이며 피셔정보(Fisher information) \\(\\bm I(\\bm \\theta)\\)는 관측피셔정보의 기대값으로 모수만의 함수로서 더이상 확률변수가 아니다. 첫 번째 방정식 (2.1)는 다음과 같이 적분과 미분의 교환에 의해 증명할 수 있다. \\[\\begin{align*} 0 &amp;= \\frac{\\partial}{\\partial \\bm \\theta } \\int f(y;\\bm \\theta) ~ dy \\\\ &amp;= \\int \\frac{\\partial}{\\partial \\bm \\theta } f(y;\\bm \\theta) ~ dy \\\\ &amp;= \\int \\frac{\\frac{\\partial}{\\partial \\bm \\theta } f(y;\\bm \\theta)}{f(y;\\bm \\theta)} f(y;\\bm \\theta) ~ dy \\\\ &amp;= \\int \\frac{\\partial}{\\partial \\bm \\theta } \\ell (\\bm \\theta;y) f(y;\\bm \\theta) ~ dy \\\\ &amp;= E \\left \\{ \\frac{\\partial}{\\partial \\bm \\theta } \\ell (\\bm \\theta;y) \\right \\} \\end{align*}\\] 두 번째 방정식 (2.2)는 아래와 같이 증명할 수 있다. \\[\\begin{align*} 0 &amp;= \\frac{\\partial}{\\partial \\bm \\theta } \\int \\frac{\\partial}{\\partial \\bm \\theta } \\ell ( \\bm \\theta;y) f(y;\\bm \\theta) ~dy \\\\ &amp; = \\int \\frac{\\partial}{\\partial \\bm \\theta } \\left \\{ f(y;\\bm \\theta) \\left [ \\frac{\\partial}{\\partial \\bm \\theta } \\ell ( \\bm \\theta;y) \\right ]^t \\right \\} ~dy \\\\ &amp;= \\int \\left \\{ \\left [ \\frac{\\partial}{\\partial \\bm \\theta } f(y;\\bm \\theta) \\right ] \\left [ \\frac{\\partial}{\\partial \\bm \\theta } \\ell (\\bm \\theta;y) \\right ]^t + f(y;\\bm \\theta) \\frac{\\partial}{\\partial \\bm \\theta } \\left [ \\frac{\\partial}{\\partial \\bm \\theta } \\ell ( \\bm \\theta;y) \\right ]^t \\right \\} ~dy \\\\ &amp;= \\int \\left \\{ \\left [ \\frac{\\partial}{\\partial \\bm \\theta } \\ell (\\bm \\theta;y) \\right ] \\left [ \\frac{\\partial}{\\partial \\bm \\theta } \\ell (\\bm \\theta;y) \\right ]^t f(y;\\bm \\theta) + \\left [ \\frac{\\partial^2}{\\partial {\\bm \\theta}^2 } \\ell (\\bm \\theta;y) \\right ] f(y;\\bm \\theta) \\right \\} ~dy \\\\ &amp;= E \\left \\{ \\left [ \\frac{\\partial}{\\partial \\bm \\theta } \\ell (\\bm \\theta;y) \\right ] \\left [ \\frac{\\partial}{\\partial \\bm \\theta } \\ell (\\bm \\theta;y) \\right ]^t \\right \\} + E \\left \\{ \\frac{\\partial^2}{\\partial {\\bm \\theta}^2 } \\ell (\\bm \\theta;y) \\right \\} \\end{align*}\\] 2.3 독립표본 표본 \\(y_1,y_2,\\dots,y_n\\) 가 분포 \\(f(y_i ; {\\bm \\theta})\\)에서 독립적으로 얻어졌다면 표본에 대한 가능도함수 \\(L_n (\\bm \\theta)\\) 은 다음과 같다. \\[\\begin{equation} L_n(\\bm \\theta) = \\prod_{i=1}^n f(y_i; {\\bm \\theta}) \\tag{2.3} \\end{equation}\\] 또한 표본에 대한 로그가능도함수 \\(\\ell_n (\\bm \\theta)\\) 은 다음과 같다. \\[\\begin{equation} \\ell_n(\\bm \\theta) = \\ell_n(\\bm \\theta; \\bm y)=\\log L_n(\\bm \\theta) = \\log \\prod_{i=1}^n f(y_i; \\bm \\theta) = \\sum_{i=1}^n \\log f(y_i; \\bm \\theta) = \\sum_{i=1}^n \\ell(\\bm \\theta; y_i) \\tag{2.4} \\end{equation}\\] 표본에 의한 로그 가능도함수 \\(\\ell_n(\\bm \\theta)\\)를 미분한 값, 즉 표본에 의한 스코어 함수 \\(s_n(\\bm \\theta)\\)는 다음과 같이 정의한다. \\[ s_n(\\bm \\theta) = \\pardiff{}{\\bm \\theta}\\ell_n(\\bm \\theta; \\bm y ) \\] \\(n\\)개의 표본에 대한 관측피셔정보 \\(\\bm J_n(\\bm \\theta)\\)와 피셔정보 \\(\\bm I_n(\\bm \\theta)\\)도 한 개의 확률 변수 경우와 유사하게 다음과 같이 정의된다. \\[ \\bm I_n(\\bm \\theta) = E \\left [ \\bm J_n(\\bm \\theta; \\bm y) \\right ] = E \\left [ -\\pardiffdd{}{\\bm \\theta}{\\bm \\theta^t}\\ell_n(\\bm \\theta; \\bm y ) \\right ] \\] 2.4 지수군 분포 확률변수 \\(y\\)가 다음과 같은형태의 분포를 따른다면 \\(y\\)의 분포는 지수군(exponential family)에 속한다고 한다. \\[\\begin{equation} f(y ; \\bm \\theta, \\phi ) = \\exp \\left \\{ \\frac{\\bm t (y)^t \\bm \\xi (\\bm \\theta)-b(\\bm \\theta)}{a(\\phi) } + c(y,\\phi) \\right \\} \\tag{2.5} \\end{equation}\\] 다시 쓰면 \\[ \\log f(y ; \\bm \\theta, \\phi ) = \\frac{\\bm t (y)^t \\bm \\xi (\\bm \\theta)-b(\\bm \\theta)}{a(\\phi) } + c(y,\\phi) \\] 지수군 분포 (2.5) 에서 \\(\\bm t (y)^t = [ t_1 (y), \\dots, t_k (y)]\\) 는 \\(k\\) 개의 충분통계량으로 구성된 벡터이고 \\(k\\)-차원 벡터 \\(\\bm \\xi (\\bm \\theta)^t = [ \\xi_1 (\\bm \\theta), \\dots, \\xi_k (\\bm \\theta) ]\\)를 기본형 모수(canonical parameter)라고 부른다. 또한 \\(a(\\phi)\\)를 스케일모수(scale parameter)라고 부르며 많은 경우 \\(a(\\phi) = a \\times w\\) 의 형태로 나타나며 여기서 \\(w\\)는 보통 가중치와 같은 역활을 한다. 또한 기본형 모수 \\(\\bm \\xi (\\bm \\theta)\\) 는 \\(y\\) 의 평균 \\(\\mu=E(y)\\)와 특별한 함수관계를 가진다. 지수군 분포는 일반적으로 식 (2.5) 과 같은 나타낼 수 있지만 많은 경우 모수 \\(\\bm \\theta\\)와 기본형 모수 \\(\\bm \\xi (\\bm \\theta)\\)는 일대일 대응관계를 가진다. 일대일 대응 관게가 아닌 경우 이를 곡선형 지수군(curved exponential family)라고 부른다. 따라서 지수군 분포의 성질을 간결하게 유도하고 설명하기 위해서 다음과 같은 단순화된 형태의 식을 사용하는 것이 편리하다. 이후 모든 성질의 유도는 아래 식 (2.6) 의 형태를 사용할 것이다. \\[\\begin{equation} \\log f(y ; \\bm \\theta, \\phi ) = \\frac{ {\\bm t}^t \\bm \\theta - b(\\bm \\theta)}{a(\\phi) } + c(y,\\phi) \\tag{2.6} \\end{equation}\\] 위의 식 (2.6) 은 충분통계량 벡터를 \\(\\bm y\\)으로 나타내고 대응하는 기본형 모수를 \\(\\bm \\theta\\)로 표시한 것이다. 또한 여기서 충분통계량 벡터 \\(\\bm t\\)의 평균을 \\(\\bm \\mu\\)라고 하자. \\[ E(\\bm t) = \\bm \\mu \\] 2.4.1 평균, 분산과 기본형 모수의 관계 이제 (2.6) 에 나오는 \\(b(\\bm \\theta)\\)의 미분을 다음과 같이 표시하자. \\[ b&#39;(\\bm \\theta ) = \\pardiff{b( \\bm \\theta)}{\\bm \\theta} , \\quad b&#39;&#39;(\\bm \\theta) = \\pardiffd{b(\\bm \\theta)}{\\bm \\theta} \\] 방정식 (2.1)으로부터 다음과 같은 식이 유도된다.. \\[\\begin{align*} 0 &amp; = E \\left ( \\pardiff{\\ell }{\\bm \\theta } \\right ) \\\\ &amp; = E \\left ( \\bm t-b&#39;(\\bm \\theta) )/ a(\\phi) \\right ) \\\\ &amp; = [ \\bm \\mu-b&#39;(\\bm \\theta) ] / a(\\phi) \\end{align*}\\] 따라서 평균 \\(\\bm \\mu\\)와 함수 \\(b(\\bm \\theta)\\)는 다음과 같은 관계가 성립한다. \\[\\begin{equation} E(\\bm t) = \\bm \\mu = b&#39;(\\bm \\theta) \\tag{2.7} \\end{equation}\\] 또한 방정식 (2.2) 으로부터 다음이 성립하고 \\[ E \\left \\{ a^{-2}(\\phi) [ \\bm t-b&#39;(\\bm \\theta)][\\bm t-b&#39;(\\bm \\theta)]^t \\right \\} + E \\left \\{ - a^{-1} (\\phi) b&#39;&#39;(\\bm \\theta) \\right \\} =0 \\] 따라서 \\(\\bm t\\)의 분산과 함수 \\(b(\\bm \\theta)\\)는 다음과 같은 관계가 성립한다. \\[\\begin{equation} Var (\\bm t) = a(\\phi) b&#39;&#39;(\\bm \\theta) \\equiv a(\\phi) v(\\bm \\mu) \\tag{2.8} \\end{equation}\\] 위의 식에서 \\(v(\\bm \\mu) = b&#39;&#39;(\\bm \\theta)\\) 으로 정의하고 분산함수(variance function)라고 부른다. 이제 분산함수의 정의로부터 \\(\\bm \\mu, \\bm \\theta\\)에 대하여 다음과 같은 관계가 얻어지고 \\[\\begin{equation} \\pardiff{\\bm \\mu}{\\bm \\theta } = \\pardiff{b&#39;(\\bm \\theta)}{\\bm \\theta } = b&#39;&#39;(\\bm \\theta) = v(\\bm \\mu) \\tag{2.9} \\end{equation}\\] 도함수의 역관계가 다음과 같이 주어진다. \\[\\begin{equation} \\frac{\\partial \\bm \\theta}{\\partial \\bm \\mu } = \\left [ \\frac{\\partial \\bm \\mu }{\\partial \\bm \\theta} \\right ]^{-1} = \\left [ b&#39;&#39;(\\bm \\theta) \\right ]^{-1} = v^{-1}(\\bm \\mu) \\tag{2.10} \\end{equation}\\] 2.4.2 지수군 분포의 예제 예제 2.1 (이항분포) 확률변수 \\(S\\) 가 이항분포 \\(B(n,\\mu)\\)를 따른다면(여기서 \\(\\mu=p\\) 성공확률) 표본 비율 \\(y=S/n\\)의 로그확률밀도함수는 다음과 같다. \\[\\begin{align*} \\log f(y ; \\theta, \\phi ) &amp; = \\log \\left \\{ \\binom{n}{ny} \\mu^{ny} (1-\\mu)^{n-ny} \\right \\} \\\\ &amp; = \\frac{ y \\log \\frac{\\mu}{1-\\mu} + \\log (1-\\mu)}{n^{-1}} + \\log \\binom{n}{ny} \\\\ &amp;= \\frac{y\\theta-b(\\theta)}{a(\\phi) } + c(y,\\phi) \\end{align*}\\] 충분통계량 \\(t\\)는 표본비율 \\(y\\)이고 기본형 모수 \\(\\theta\\)와 평균 \\(E(y)=\\mu=p\\), 스케일 모수 \\(a(\\phi)\\)은 다음과 같은 관계가 있다. \\[\\begin{equation} \\theta = \\log \\frac{\\mu}{1-\\mu} = \\log \\frac{p}{1-p} , \\quad b(\\theta) = - \\log(1-\\mu), \\quad a(\\phi) = \\frac{1}{n} \\tag{2.11} \\end{equation}\\] 평균 \\(\\mu\\)를 기본형모수 \\(\\theta\\)의 함수로 역변환하면 \\(\\theta\\)의 로지스틱함수로 표현된다. \\[\\begin{equation*} \\mu = \\frac{\\exp(\\theta)}{1+\\exp(\\theta)}=\\frac{1}{1+\\exp(-\\theta)}, \\quad 1-\\mu = \\frac{1}{1+\\exp(\\theta)} \\end{equation*}\\] 또한 함수 \\(b(\\theta)\\)를 기본형 모수로 나타내면 \\[\\begin{equation*} b(\\theta) = - \\log ( 1-\\mu) = \\log [ 1+\\exp(\\theta)] \\end{equation*}\\] 특별히 \\(n=1\\)인 경우는 베르누이 분포이다. 이항분포에서 평균 \\(\\mu\\)는 함수 \\(b\\)와 다음과 같은 관계가 있다. \\[\\begin{equation*} b&#39;(\\theta) = \\frac{\\exp(\\theta)}{1+\\exp(\\theta)} =\\mu \\end{equation*}\\] 또한 \\[\\begin{equation*} b&#39;&#39;(\\theta) = \\frac{\\exp(\\theta)[1+\\exp(\\theta)] -[\\exp(\\theta)]^2 }{[1+\\exp(\\theta)]^2} =\\mu(1-\\mu) = v(\\mu) \\end{equation*}\\] 따라서 \\[\\begin{equation*} Var(y) = a(\\phi) v(\\mu) =\\frac{\\mu(1-\\mu)}{n} \\end{equation*}\\] 예제 2.2 (포아송분포) 확률변수 \\(y\\) 가 포아송 분포 \\(poi(\\mu)\\)를 따른다고 하자. 여기서 \\(E(y) = \\mu\\) 이다. 확률변수 \\(y\\)의 로그확률밀도함수는 다음과 같다. \\[ \\log f(y ; \\theta, \\phi ) = y \\log \\mu -\\mu - log(y!) \\] 충분통계량 \\(t\\)는 반응값 \\(y\\) 자체이고 기본형 모수 \\(\\theta\\)와 평균 \\(E(y)=\\mu\\), 스케일 모수 \\(a(\\phi)\\)은 다음과 같은 관계가 있다.기본형 모수 \\(\\theta\\)와 평균 \\(\\mu\\), 스케일 모수 \\(a(\\phi)\\)은 다음과 같은 관계가 있다. \\[\\begin{equation} \\theta = \\log \\mu , \\quad b(\\theta) = \\mu , \\quad a(\\phi) = 1 \\tag{2.12} \\end{equation}\\] 평균 \\(\\mu\\)를 기본형모수 \\(\\theta\\)의 함수로 역변환하면 \\(\\theta\\)의로그함수로 표현된다. 또한 함수 \\(b(\\theta)\\)를 기본형 모수로 나타내면 다음과 같다. \\[ \\mu = \\exp(\\theta), \\quad b(\\theta) = \\mu =\\exp(\\theta) \\] 따라서 포아송분포에서는 평균 \\(\\mu\\)는 함수 \\(b\\)와 다음과 같은 관계가 있다. \\[ b&#39;(\\theta) = \\exp(\\theta) =\\mu, \\quad b&#39;&#39;(\\theta) = \\exp(\\theta) =\\mu = v(\\mu) \\] 따라서 \\[ Var(y) = a(\\phi) v(\\mu) =\\mu \\] 예제 2.3 (정규분포) 확률변수 \\(y\\) 가 정규분포 \\(N(\\mu, \\sigma^2)\\)를 따른다고 하자. 확률변수 \\(y\\)의 로그확률밀도함수는 다음과 같다. 이때 모수벡터를 \\({\\bm \\theta}^t =(\\mu,\\sigma^2)\\)이다. \\[\\begin{align*} \\log f(y ; \\bm \\theta, \\phi ) &amp; = -\\frac{1}{2} \\log (2 \\pi)-\\frac{1}{2} \\log \\sigma^2 -\\frac { (y - \\mu)^2 }{2\\sigma^2} \\\\ &amp;= -\\frac{1}{2} \\log (2 \\pi)-\\frac{1}{2} \\log \\sigma^2 -\\frac { y^2 -2 y \\mu + \\mu^2 }{2\\sigma^2} \\\\ &amp; = \\left [ y \\frac{\\mu}{\\sigma^2} - y^2 \\frac{1}{2\\sigma^2} \\right ] - \\left [ \\frac{1}{2} \\log \\sigma^2 + \\frac{\\mu^2}{2\\sigma^2} \\right ] -\\frac{n}{2} \\log (2 \\pi) \\end{align*}\\] 충분 통계량과 기본형 모수는 다음과 같다. \\[ \\bm t(y)^t = ( y, y^2), \\quad \\bm \\xi (\\bm \\theta)^t = \\left ( \\frac{\\mu}{\\sigma^2}, -\\frac{1}{2\\sigma^2} \\right ), \\quad a(\\phi)=1 \\] 또한 \\[ b(\\bm \\theta) = \\frac{1}{2} \\log \\sigma^2 + \\frac{\\mu^2}{2\\sigma^2} \\] 예제 2.4 (다항분포) 다항분포(multinomial distribution)인 경우도 고려해 보자. 확률벡터 \\(y^t=(y_1,\\dots,y_k)\\)를 성공확률이 \\(\\mu^t = (\\mu_1,\\dots, \\mu_k)\\) 인 다항분포를 따른다고 하자 (\\(\\sum_j \\mu_j=1\\), \\(\\sum y_j =m\\)). 확률벡터 \\(y\\)의 확률함수는 다음과 같다. \\[\\begin{equation*} \\log f(y, \\mu, m) = \\log \\frac{m!}{y_1!\\dots y_k!} \\mu_1^{y_1} \\mu_2^{y_2} \\dots \\mu_k^{y_k} = \\sum_{j=1}^{k-1} y_j \\log \\tfrac{\\mu_j}{1-\\sum_1^{k-1} \\mu_j} +m \\log(1-\\sum_1^{k-1} \\mu_j) +c(m,y) \\end{equation*}\\] 2.4.3 최대가능도추정법 모수 \\(\\bm \\theta\\) 에 대한 최대가능도 추정량(Maximum Likelihood Estimator;MLE) \\(\\hat {\\bm \\theta}\\)는 가능도 함수를 최대로 하는 값으로 정의된다. \\[ \\hat {\\bm \\theta}_{MLE} = \\arg \\max_{\\bm \\theta} L_n(\\bm \\theta) \\] 많은 경우 가능도 함수를 최대화하는 값을 구하기 어려우므로 가능도 함수의 로그 함수, 즉 로그가능도함수를 최대로 하는 값으로 최대가능도 추정량을 구한다. \\[ \\hat {\\bm \\theta}_{MLE} = \\arg \\max_{\\bm \\theta} \\ell_n(\\bm \\theta) \\] 만약 로그가능도 함수가 모수 \\(\\bm \\theta\\)에 대하며 미분가능한 함수이면 최대가능도 추정량은 다음과 같은 방정식에 의하여 구할 수 있다. \\[ \\pardiff{}{\\bm \\theta}\\ell_n(\\bm \\theta; \\bm y ) = s_n(\\bm \\theta)=\\bm 0 \\] 최대가능도 추정량은 적당한 조건하에서 다음과 같은 점근적 성질(Asymptotical properties)을 가진다. \\(\\hat {\\bm \\theta}_{MLE}\\)는 모수의 참값 \\(\\bm \\theta_0\\)로 확률적 수렴한다. \\[ \\hat {\\bm \\theta}_{MLE} \\rightarrow_p \\bm \\theta_0 \\quad \\text{as } n \\rightarrow \\infty \\] 최대가능도추정량 \\(\\hat {\\bm \\theta}_{MLE}\\)는 점근적으로 정규분포를 따른다. \\[ \\hat {\\bm \\theta}_{MLE} \\sim_d N(\\bm \\theta_0, \\bm I_n^{-1}(\\bm \\theta_0)) \\] 2.5 선형모형 반응변수가 \\(y\\)이고 \\(p-1\\)개의 독립변수 \\((x_1, x_2, \\cdots, x_{p-1})\\)가 있다고 가정하고 표본의 크기 \\(n\\)인 자료가 얻어지면 선형회귀식을 행렬로 다음과 같이 표현할 수 있다. \\[\\begin{eqnarray*} y_i &amp; = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\cdots + \\beta_kx_{i,p-1} + e_i \\\\ &amp; = \\bm x^t_i \\bm \\beta + e_i, \\quad i=1,2,\\cdots,n \\end{eqnarray*}\\] 위의 식을 다시 표현하면 다음과 같이 쓸 수 있다. 이와 같은 회귀모형을 선형중회귀모형이라 부르며. 각 개체에 대한 모형의 방정식을 하나의 식으로 표현하면 다음과 같다. 위의 선형모형(linear model)을 벡터와 행렬을 이용하여 표시하면 다음과 같다. \\[\\begin{equation} \\bm y=\\bm X \\bm \\beta+ \\bm e \\tag{2.13} \\end{equation}\\] 여기서 \\(\\bm y\\) 는 \\(n \\times 1\\) 반응변수 벡터, \\(\\bm X\\) 는 \\(p\\) 개의 독립변수로 이루어진 \\(n \\times p\\) 계획행렬(design matrix)이다. 모수 벡터 \\(\\bm \\beta\\) 는 \\(p \\times 1\\) 벡터로 각 독립변수에 대한 회귀계수 벡터이다. \\(\\bm e\\)는 \\(n \\times 1\\) 오차벡터이다. 여기서 회귀분석의 오차항의 가정을 살펴보면 오차항이 서로 독립이고 동일한 분산을 갖는다. 즉, 오차항은 다음의 분포를 따른다. 즉, \\(\\bm e \\sim (0,\\sigma^2\\bm I)\\). 관측값 벡터 \\(\\bm y\\)의 평균과 분산을 보면 \\[\\begin{equation*} E(\\bm y|\\bm X) = \\bm X \\bm \\beta, \\quad Var(\\bm y|\\bm X) = \\sigma^2 \\bm I \\end{equation*}\\] 여기서 오차항이 정규분포를 따른다면 (\\(\\bm e \\sim N(0,\\sigma^2 \\bm I)\\) 관측값 벡터 \\(\\bm y\\) 또한 정규분포를 따른다 \\[ \\bm y \\sim N(\\bm X \\bm \\beta, \\sigma^2 \\bm I) \\] 또한 \\(X\\) 가 완전계수(full rank) 행렬이라고 가정하자. \\(p+1\\)개의 모수를 모아놓은 모수벡터는 \\(\\bm \\theta=(\\bm \\beta^t, \\sigma^2)^t\\)이다. 여기서 편의상 오차항의 분산을 \\(\\tau=\\sigma^2\\) 로 표시하하고자 한다, 즉 \\(\\bm \\theta=(\\bm \\beta^t, \\tau)^t\\) 2.5.1 최소제곱 추정 위의 선형 모형 가정하에서, 최소제곱 추정량 \\(\\hat {\\bm \\beta}\\) (least square estimator)는 다음과 같이 오차제곱합(Error Sum of Sqaures) \\(SSE\\) 를 최소로 하는 추정량이다. \\[\\begin{align*} SSE(\\bm \\beta) &amp; = \\sum_{i=1}^n (y_i - \\bm x_i^t \\bm \\beta)^2 \\\\ &amp;= (\\bm y - \\bm X \\bm \\beta)^t(\\bm y - \\bm X \\bm \\beta) \\\\ \\hat {\\bm \\beta} &amp; = \\arg \\min_{\\bm \\beta} SSE(\\bm \\beta) \\end{align*}\\] 따라서 \\(\\hat {\\bm \\beta}\\)는 오차제곱합을 최소로 하는 계수 벡터이며 최소제곱 추정량은 다음과 같이 주어진다. \\[ \\hat {\\bm \\beta}_{LS} = (\\bm X^t \\bm X)^{-1} \\bm X^t \\bm y \\] 오차제곱합에 최소제곱 추정량을 사용하면 이를 잔차제곱합(Residual Sum of Squares)라고 하며 이를 \\(SSE(\\hat {\\bm \\beta})\\)로 표시한다. \\[ SSE(\\hat {\\bm \\beta}) = (\\bm y - \\bm X \\hat {\\bm \\beta})^t(\\bm y - \\bm X \\hat {\\bm \\beta}) \\] 오차항의 분산에 대한 추정은 정규분포 가정을 오차항에 대한 정규분포를 가정하고 다음과 같은 잔차제곱합의 분포에 대한 결과를 이용하면 \\[ SSE(\\hat {\\bm \\beta}) \\sim \\sigma^2 \\chi^2 (n-p) \\] 오차항의 분산 \\(\\sigma^2\\)의 불편추정량 \\(S^2\\)을 구할 수 있다. \\[ S^2 = \\frac{ SSE(\\hat {\\bm \\beta}) }{ (n-p)} =\\frac{ \\sum_{i=1}^n (y_i - \\bm x_i^t \\hat {\\bm \\beta} )^2 }{ (n-p)} \\] 즉, \\[ E(S^2) = \\sigma^2 \\] 여기서 자유도 \\(n-p\\)은 자료의 개수 \\(n\\)에서 절편을 포함한 회귀계수의 개수 \\(p\\)를 뺀 수이다. 2.5.2 가능도함수 선형모형 (2.13)에 대한 가능도 함수는 다음과 같이 주어진다. \\[\\begin{align*} L_n(\\bm \\theta ; \\bm y) &amp; = L(\\bm \\beta,\\sigma^2| \\bm y) \\\\ &amp; = \\prod^n_{i=1} f(y_i)\\\\ &amp; = \\prod^n_{i=1}(2 \\pi \\sigma^2)^{-\\frac{1}{2}} \\exp \\left [-\\frac{1}{2\\sigma^2} (y_i-\\bm x_i^t\\bm \\beta)^2 \\right ] \\\\ &amp; = (2\\pi\\sigma^2)^{-\\frac{n}{2}} \\exp \\left [ -\\frac{1}{2\\sigma^2}(\\bm y-\\bm X \\bm \\beta)^t(\\bm y-\\bm X \\bm \\beta) \\right ] \\end{align*}\\] 또한 분산에 대한 모수를 \\(\\tau=\\sigma^2\\) 과 같이 쓰면 로그 가능도함수는 다음과 같다. \\[\\begin{align*} \\ell_n(\\bm \\theta;\\bm y) &amp; = -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\sigma^2 -\\frac { (\\bm y-\\bm X \\bm \\beta)^t (\\bm y-\\bm X \\bm \\beta) }{2\\sigma^2} \\\\ &amp;= -\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\tau -\\frac { (\\bm y-\\bm X \\bm \\beta)^t (\\bm y-\\bm X \\bm \\beta) }{2\\tau} \\end{align*}\\] 이제 로그가능도함수로부터 구할 수 있는 스코어함수 \\(s(\\bm \\theta;y)\\) 와 그에 대한 관측 피셔정보 \\(\\bm J_n(\\bm \\theta; \\bm y)\\) 은 다음과 같이 주어진다. \\[\\begin{align*} s(\\bm \\theta; \\bm y) &amp; = \\pardiff{}{\\bm \\theta}\\ell_n(\\bm \\theta; \\bm y ) \\\\ &amp; = \\begin{bmatrix} \\pardiff{}{\\bm \\beta}\\ell_n(\\bm \\theta; \\bm y ) \\\\ \\pardiff{}{\\tau}\\ell_n(\\bm \\theta; \\bm y ) \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} \\bm X^t (\\bm y-\\bm X \\bm \\beta)/\\tau \\\\ -\\frac{n}{2\\tau} +\\frac { (\\bm y-\\bm X \\bm \\beta)^t (\\bm y-\\bm X \\bm \\beta) }{2\\tau^2} \\end{bmatrix} \\\\ \\bm J_n(\\bm \\theta; \\bm y) &amp; = -\\pardiffdd{}{\\bm \\theta}{\\bm \\theta^t}\\ell_n(\\bm \\theta;y ) \\\\ &amp; = - \\begin{bmatrix} \\pardiffdd{}{\\beta}{\\bm \\beta^t}\\ell_n(\\bm \\theta;y ) &amp; \\pardiffdd{}{\\bm \\beta}{\\tau^t}\\ell_n(\\bm \\theta;y ) \\\\ \\pardiffdd{}{\\tau}{\\bm \\beta^t}\\ell_n(\\bm \\theta;y ) &amp; \\pardiffdd{}{\\tau}{\\tau^2}\\ell_n(\\bm \\theta;y ) \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} \\bm X^t \\bm X /\\tau &amp; \\bm -\\bm X^t (\\bm y-\\bm X \\bm \\beta)/\\tau^2 \\\\ -\\bm X^t (\\bm y-\\bm X \\bm \\beta)/\\tau^2 &amp; - \\frac{n}{2\\tau^2} +\\frac { (\\bm y-\\bm X \\bm \\beta)^t (\\bm y-\\bm X \\bm \\beta) }{\\tau^3} \\end{bmatrix} \\end{align*}\\] 2.5.3 최대가능도 추정량 이제 회귀계수 \\(\\bm \\beta\\)에 대한 최대가능도 추정량은 스코어함수로 부터 얻어진 방정식 \\(s(\\bm \\theta;\\bm y)=\\bm 0\\) 으로부터 얻어지며 다음과 같은 형태를 가진다. \\[ \\hat {\\bm \\beta} = (\\bm X^t \\bm X)^{-1} \\bm X^t \\bm y \\] \\[ \\hat \\sigma^2 = \\hat \\tau = (\\bm y-\\bm X \\bm {\\hat \\beta})^t (\\bm y-\\bm X \\bm {\\hat \\beta})/n = \\frac{SSE(\\hat{\\bm \\beta})}{n} \\] 여기서 유의할 점은 회귀계수 \\(\\bm \\beta\\) 의 최대가능도 추정량은 최소제곱법으로 구한 추정량과 동일하다. 따라서 \\(\\hat {\\bm \\beta}\\)은 최소분산 불편 추정량이다. 하지만 오차항의 분산 \\(\\sigma^2\\) 에 대한 최대가능도 추정량은 불편추정량이 아니다. \\[ E(\\hat \\sigma^2) = E \\left [ (\\bm y-\\bm X \\bm {\\hat \\beta})^t (\\bm y-\\bm X \\bm {\\hat \\beta})/n \\right ] = E \\left [ \\frac{SSE}{n} \\right ] \\ne \\sigma^2 \\] 참고로 오차항의 분산 \\(\\sigma^2\\)에 대한 불편추정량은 \\(SSE/(n-p)\\)이다. 최대가능도 추정량의 점근적 분포를 이용하면 다음과 같이 말할 수 있다. 오차항이 정규분포인 선형모형인 경우 아래의 분포는 점근분포가 아닌 정확한 분포이다. \\[ \\hat {\\bm \\theta} - \\bm \\theta_0 \\sim N(\\bm 0, \\bm I_n^{-1}(\\bm \\theta_0)) \\] 여기서 \\[ \\bm I_n(\\bm \\theta) = E[\\bm J(\\bm \\theta; \\bm y)] = \\begin{bmatrix} \\bm X^t\\bm X /\\tau &amp; 0 \\\\ 0 &amp; \\frac{n}{2\\tau^2} \\end{bmatrix} \\] 그리고 \\[ \\bm I_n^{-1}(\\bm \\theta) = \\begin{bmatrix} \\tau(\\bm X^t \\bm X)^{-1} &amp; 0 \\\\ 0 &amp; \\frac{2\\tau^2}{n} \\end{bmatrix} = \\begin{bmatrix} \\sigma^2(\\bm X^t \\bm X)^{-1} &amp; 0 \\\\ 0 &amp; \\frac{2\\sigma^4}{n} \\end{bmatrix} \\] 따라서 회귀계수 \\(\\hat {\\bm \\beta}- \\bm \\beta_0\\)의 분포는 평균이 \\(\\bm 0\\) 이고 공분산이 \\(\\sigma^2(\\bm X^t \\bm X)^{-1}\\) 인 정규분포를 따른다. 여기거 주목할 점은 가능도함수에 최대가능도추정량을 대입하면 그 값이 \\(SSE(\\hat {\\bm \\beta})\\)의 함수로 나타난다. \\[\\begin{align*} L_n(\\hat {\\bm \\theta} ) &amp; = L_n(\\hat {\\bm \\beta} ,\\hat \\sigma^2 ) \\\\ &amp;= (2\\pi\\hat \\sigma^2)^{-\\frac{n}{2}} \\exp \\left [-\\frac{1}{2 \\hat \\sigma^2}(\\bm y-\\bm X \\hat {\\bm \\beta})^t(\\bm y-\\bm X \\hat {\\bm \\beta} ) \\right ] \\\\ &amp; = (2\\pi\\hat \\sigma^2)^{-\\frac{n}{2}} \\exp \\left [-\\frac{n}{2} \\right ] \\\\ &amp; = \\left (2\\pi \\frac{SSE(\\hat {\\bm \\beta})}{n} \\right )^{-\\frac{n}{2}} \\exp \\left [-\\frac{n}{2} \\right ] \\\\ l_n(\\hat {\\bm \\theta} ) &amp;= l_n(\\hat {\\bm \\beta} ,\\hat \\sigma^2 ) \\\\ &amp;= \\text{constant} - \\frac{n}{2} \\log SSE(\\hat {\\bm \\beta}) \\end{align*}\\] 따라서 잔차제곱함 \\(SSE(\\hat {\\bm \\beta})\\) 작아지면 가능도함수는 커진다. 2.6 최대가능도 추정량의 점근적 성질 (update 필요) 로그가능도 함수의 테일러 전개를 고려해 보자. \\[ \\ell(\\bm \\theta) = \\ell(\\hat {\\bm \\theta}) + s(\\hat {\\bm \\theta})^t({\\bm \\theta} - \\hat {\\bm \\theta}) + \\frac{1}{2} ({\\bm \\theta} - \\hat {\\bm \\theta})^t \\nabla(\\hat {\\bm \\theta}) ({\\bm \\theta} - \\hat {\\bm \\theta}) + o_p(1) \\] 따라서 \\[ 2 (\\ell({\\bm \\theta}) = \\ell(\\hat {\\bm \\theta}) + 0 - \\frac{1}{2} ({\\bm \\theta} - \\hat {\\bm \\theta})^t I({\\bm \\theta})({\\bm \\theta} - \\hat {\\bm \\theta}) +\\frac{1}{2} ({\\bm \\theta} - \\hat {\\bm \\theta})^t [\\nabla(\\hat {\\bm \\theta})+I({\\bm \\theta})] ({\\bm \\theta} - \\hat {\\bm \\theta}) + o_p(1) \\] "],
["chapglm.html", "제 3 장 일반화 선형모형 3.1 일반화선형모형 3.2 일반화 선형모형의 가능도함수 3.3 최대가능도추정 3.4 최대가능도추정량의 계산 3.5 Maximum Quasi-Likelihood", " 제 3 장 일반화 선형모형 3.1 일반화선형모형 1 장에서 살펴본 회귀모형을 보통 선형모형이라고 부른다. 선형모형의 의미는 모형에서 고려하는 설명변수가 변할 때 반응값의 평균이 변하는 관계가 선형이라는 것이다. 즉, 반응값의 평균을 설명하는 회귀식이 회귀계수에 대하여 선형이라는 의미이다. 참고로 아래 식 (2.13)의 오른쪽에 나타나는 식을 선형예측식(linear predictor, \\(\\eta\\))라고 부른다. \\[\\begin{equation} E(y|x_1,x_2,\\dots,x_p) = \\beta_ 1 x_1 + \\dots + \\beta_p x_p \\equiv \\eta \\tag{2.13} \\end{equation}\\] 이러한 선형성의 가정이 적절하지 않은 경우가 있다. 예를 들어 공학에서나 생물학에서 사용되는 비선형 회귀모형(nonlinear regression model)처럼 반응변수의 변화가 설명변수들의 복잡한 비선형 관계(예를 들어 미분방정식의 관계)로 나타나는 경우로 흔히 나타난다. 또한 반응변수가 가질 수 있는 평균값에 제한이 있을 수 있다. 예를 들어 베르누이 분포의 경우 평균이 성공확률이기 때문에 0과 1사이에 있으며 포아송 분포의 경우 반응값은 음의 값을 가질 수 없다. 따라서 식 (2.13)의 선형예측식 \\(\\eta\\)와 반응값의 평균 \\(E(y|\\bm x)\\)의 관계를 선형모형 (2.13)처럼 정의할 수 없다. 이렇게 반응변수의 평균과 선형에측식의 범위가 일치하지 않는 경우 임의의 단조증가 함수 \\(g\\)를 사용하여 그 범위를 일치하게 만들어 줄 수 있다. 예를 들어 베르누이 분포의 경우 표준 정규분포의 누적분포함수 \\(\\Phi\\)를 사용하여 확률의 범위와 선형에측식의 범위를 맞추어 줄 수 있다. 이러한 회귀모형을 프로빗(probit)모형이라고 부른다. \\[\\begin{equation} \\Phi^{-1} [p(y|\\bm x)] = \\beta_ 1 x_1 + \\dots + \\beta_p x_p =\\eta \\tag{3.1} \\end{equation}\\] 이제부터 정규분포하에서 정의되는 선형모형을 다른 분포들로 확장한 모형인 일반화 선형모형(Generalized Linear Model; GLM)을 살펴보기로 하자. 3.1.1 지수군 분포와 일반화 선형모형 일변량 확률변수 \\(y\\)가 식 (2.6)와 같이 다음과 같은 지수군 분포를 따른다고 가정하자. \\[ f(y | \\theta, \\phi ) = \\exp \\left \\{ \\frac{y\\theta-b(\\theta)}{a(\\phi) } + c(y,\\phi) \\right \\} \\] 충분통계량이 관측값 \\(y\\)이고 1차원의 기본형 모수\\(\\theta\\)로 정의된 분포임을 유의하자. 확률변수 \\(y\\)의 평균을 \\(\\mu = E(y)\\) 이라고 하고 독립변수 벡터 \\(\\bm x\\)와 회귀계수 벡터 \\(\\bm \\beta\\)로 구성된 선형예측식을 \\(\\eta\\)라고 하자. \\[\\begin{equation} \\eta = {\\bm x}^t \\bm \\beta \\tag{3.2} \\end{equation}\\] 일반화 선형모형은 분포의 특성에 따라 주어진 단조증가함수 \\(g\\) 를 이용하여 \\(y\\) 의 평균과 선형예측식의 관계를 설정하는 모형이다. 이러한 함수 \\(g\\) 를 연결함수(link function)라고 부른다. \\[\\begin{equation} g(\\mu) = g(E[y| \\bm x]) = {\\bm x}^t \\bm \\beta \\tag{3.3} \\end{equation}\\] 반응값의 분포가 주어진 경우 연결함수는 평균의 범위와 선형에측식의 범위를 연속적으로 1-1 대응하게 해주는 함수이며 사용할 수 있는 가능한 함수는 무한히 많다. 예를 들어 베르부이 분포의 경우 위에서 정의된 프로빗 모형 (3.1)에서 \\(\\Phi^{-1}\\) 같이 (0,1) 에서 실수전체 집합으로 단조증가하는 함수는 모두 연결함수로 고려할 수 있다. 지수군 분포에서 모수 \\(\\theta\\)를 기본형모수(canonical parameter)라고 부르며 일반적으로 \\(\\theta\\) 는 평균 \\(\\mu\\)의 비선형 함수로 나타난다. 만약 다음과 같은 관계를 나타내는 연결함수 \\(g\\) 가 있다면 그 함수를 기본형 연결함수(canonical link function)이라고 부른다. \\[\\begin{equation} \\theta = \\eta \\tag{3.4} \\end{equation}\\] 예를 들어 예제 2.1을 보면 이항분포의 확률밀도함수를 지수군 분포의 형태로 표현했을 때 식 (2.11)의 형태를 보면 다음과 같은 관계를 알 수 있다. \\[\\theta = \\log \\frac{\\mu}{1-\\mu} = \\log \\frac{p}{1-p} \\] 따라서 식 (3.4) 을 만족하는 연결함수는 다음과 같고 \\[\\begin{equation} \\log \\frac{p}{1-p} = \\eta = {\\bm x}^t \\bm \\beta \\tag{3.5} \\end{equation}\\] 이를 로짓 연결함수(logit link function)이라고 부르며 이는 이항분포의 기본형 연결함수이다. 만약 \\(y\\)의 분포가 정규분포이며 연결함수 \\(g\\)가 \\(g(\\mu) = \\mu\\)이면 선형회귀모형이 된다. \\[ E[y|\\bm x] = {\\bm x}^t \\bm \\beta \\] 3.2 일반화 선형모형의 가능도함수 하나의 표본 \\(y\\)에 대하여 기본형 모수 \\(\\theta\\) 하나인 로그가능도함수(log likelihood function) \\(\\ell\\)은 다음과 같이 정의된다. \\[ \\ell = \\log f(y) = \\frac{y\\theta-b(\\theta)}{a(\\phi) } + \\log c(y, \\phi) \\] 표본 \\(y_1,y_2,\\dots,y_n\\) 가 각각 설명변수 벡터\\({\\bm x}_1, {\\bm x}_2, \\dots,{\\bm x}_n\\)에서 독립적으로 얻어졌다면 로그가능도함수 \\(\\ell_n\\) 은 다음과 같다. \\[\\begin{equation} \\ell_n = \\log \\prod_{i=1}^n f(y_i) = \\sum_{i=1}^n \\frac{y_i \\theta_i-b(\\theta_i)}{a(\\phi_i) } + \\sum_{i=1}^n \\log c(y_i, \\phi) \\tag{3.6} \\end{equation}\\] 여기서 \\(\\theta_i\\)는 \\(i\\) 번째 관측값에 대한 모수로서 첨자 \\(i\\) 를 붙이는 이유는 관측치의 기대값 \\(\\mu_i = E(y_i | {\\bm x}_i)\\)가 독립변수의 값 \\({\\bm x}_i\\)에 따라 다를 수 있고 \\(\\theta_i\\)는 평균 \\(\\mu_i\\)의 함수이기 떄문이다. 이제 식 (3.3)과 같이 설명변수와 반응변수 평균과의 관계가 연결함수 \\(g\\)로 정의되었다고 하자. \\[\\begin{equation} g(\\mu_i) = g(E[y_i| {\\bm x}_i ]) = {\\bm x}_i^t \\bm \\beta \\equiv \\eta_i, \\quad i=1,2,\\dots,n \\tag{3.7} \\end{equation}\\] Nelder and Wedderburn (1972) 에서 연결 함수(link function)의 개념이 제시할 때 다음과 같은 작업 변량(working variate) \\(z_i\\)를 이용하여 선형모형을 일반화하고자 하였다. 즉, 작업 변량 \\(z_i\\) \\[\\begin{align} z_i &amp; = g(\\mu_i ) + g_\\mu(\\mu)(y_i - \\mu_i) \\\\ &amp; = {\\bm x}_i^t \\bm \\beta + g_\\mu(\\mu)(y_i - \\mu_i) \\\\ &amp; \\simeq {\\bm x}_i^t \\bm \\beta + r_i \\end{align}\\] 위의 식에서 오른쪽 식의 두번째 항의 기대값이 0이므로 이를 오차항과 같이 생각하면 위의 모형을 오차항의 분산이 다른 선형모형으로 생각할 수 있다. 지수군 분포의 성질 (2.8)를 이용하면 작업 변량 \\(z_i\\)의 분산은 다음과 같다. \\[\\begin{equation} Var(z_i) = Var(r_i) = [g_\\mu(\\mu_i)]^2 Var(y_i) = [g_\\mu(\\mu_i)]^2 [ a(\\phi_i)v(\\mu_i)] \\tag{3.8} \\end{equation}\\] 이러한 가정하에서 작업 변량 \\(z_i\\)를 반응변수로 놓고 분산의 역수를 가중치로하는 가중 선형모형(wighted linear regression) 을 최소제곡법으로 적합하고 계수의 값이 수렴할 때까지 반복적으로 수행하는하는 계산법을 제공하였다. 이러한 방법을 반복가중최소최곱법(iterative weighted least square method; IWLS)라고 부른다. (Searle and McCulloch 2001 의 136 쪽 참조) 3.3 최대가능도추정 이제 회귀계수 \\(\\bm \\beta\\)를 최대가능도추정법(Maximum Likelihood Estimation)으로 구하기 위하여 로그가능도함수 (3.6) 를 회귀계수 벡터 \\(\\bm \\beta\\)로 미분한 가능도함수 방정식을 고려하자. \\[\\begin{equation} \\pardiff{ \\ell_n }{ \\bm \\beta} = \\bm 0 \\tag{3.9} \\end{equation}\\] 여기서 일반화 선형모형에서 나타나는 모수들 \\(\\bm \\beta\\), \\(\\mu_i\\), \\(\\theta_i\\)의 관계를 살펴보자. 회귀계수 벡터 \\(\\bm \\beta\\)는 설명변수 벡터 \\({\\bm x}_i\\)와 내적 형태로 연결되어 있으며 이를 선형 예측식 이라고 한다. \\[ \\eta_i = {\\bm x}^t_i \\bm \\beta \\] 선형 예측식 \\(\\eta_i\\)는 관측값의 평균 \\(\\mu_i\\)와 연결함수 \\(g\\)로 연결되어 있다. \\[ g(\\mu_i) = \\eta_i \\] 관측값의 평균 \\(\\mu_i\\)는 기본형 모수 \\(\\theta_i\\)와 함수 \\(b\\)로 연결되어 있다. \\[ b&#39;(\\theta_i) = \\mu_i \\] 일반화 선형모형에서 최종적으로 추정해야 하는 모수는 회귀계수 벡터 \\(\\bm \\beta\\) 이며 모수 \\(\\bm \\beta\\), \\(\\mu_i\\), \\(\\theta_i\\) 다음과 연결되어 있음을 알 수 있다. \\[\\begin{equation} \\bm \\beta \\underset{ g}{\\longrightarrow} \\mu_i \\underset{ b }{\\longrightarrow} \\theta_i \\tag{3.10} \\end{equation}\\] 최대가능도 추정량을 구하는 방정식을 유도할 때 다음과 같은 일반화 선형모형에 대한 tp 가지 지수군 분포에서 나타나는 미분공식이 적용된다. 선형 예측식의 미분: \\[ \\pardiff{ \\eta }{ \\bm \\beta } = \\pardiff{ {\\bm x}^t \\bm \\beta }{ \\bm \\beta } = \\bm x \\] 연결함수의 역함수에 대한 미분: \\(g(\\mu) = \\eta\\) 의 관계를 이용하면 \\[ \\pardiff{\\mu}{\\eta} = \\pardiff{\\mu}{g(\\mu)} = \\left [ \\pardiff{g(\\mu)}{\\mu} \\right ]^{-1} = = [g&#39;(\\mu)]^{-1} = g^{-1}_{\\mu} (\\mu) \\] 여기서 \\(g_{\\mu}(\\mu) = g&#39;(\\mu)\\)로서 연결함수의 미분을 나타내는 기호이다. 평균과 기본형모수의 미분, 분산함수: 식 (2.9)에서 관계식을 이용하면 \\[\\begin{equation} \\frac{\\partial \\theta }{\\partial \\mu } = \\left [ \\frac{\\partial \\mu }{\\partial \\theta } \\right ]^{-1} = \\left [ b&#39;&#39;(\\theta) \\right ]^{-1} = \\frac{1}{v(\\mu)} \\end{equation}\\] 3.3.1 가능도 방정식의 유도: 첫 번째 방법 이제 가능도 함수 (3.6) 의 형태를 이용하여 방정식 (3.9) 를 을 유도해 보자. \\[\\begin{align} \\bm 0 &amp; =\\pardiff{ \\ell_n}{ \\bm \\beta }\\\\ &amp;= \\sum_{i=1}^n \\left [ \\frac{1}{a(\\psi_i)} \\right ] \\left [ y_i \\pardiff{ \\theta_i}{ \\bm \\beta } -\\pardiff{ b(\\theta_i)}{\\bm \\beta } \\right ] \\\\ &amp;= \\sum_{i=1}^n \\left [ \\frac{1}{a(\\psi_i)} \\right ] \\left [ y_i \\pardiff{ \\theta_i}{ \\bm \\beta } -\\pardiff{ \\theta_i }{ \\bm \\beta } \\pardiff{ b(\\theta_i)}{ \\theta_i } \\right ] \\\\ &amp;= \\sum_{i=1}^n \\left [ \\frac{1}{a(\\psi_i)} \\right ] \\left [ \\pardiff{ \\theta_i }{ \\bm \\beta } (y_i - \\mu_i) \\right ] \\\\ &amp;= \\sum_{i=1}^n \\left [ \\frac{1}{a(\\psi_i)} \\right ] \\left [ \\pardiff{ \\eta_i }{ \\bm \\beta } \\pardiff{ \\mu_i }{ \\eta_i} \\pardiff{ \\theta_i }{ \\mu_i}(y_i - \\mu_i) \\right ] \\\\ &amp;= \\sum_{i=1}^n \\left [ \\frac{1}{a(\\psi_i)} \\right ] \\left [ {\\bm x}_i \\frac{ (y_i - \\mu_i) }{ v(\\mu_i) g_\\mu(\\mu_i) } \\right ] \\\\ &amp;= \\sum_{i=1}^n \\left [ \\bm x_i w_i g_\\mu(\\mu_i) (y_i - \\mu_i) \\right ] \\\\ &amp; = \\begin{bmatrix} \\sum_{i=1}^n x_{i1} w_i g_{\\mu}(\\mu_i) (y_i - \\mu_i) \\\\ \\sum_{i=1}^n x_{i2} w_i g_{\\mu}(\\mu_i) (y_i - \\mu_i) \\\\ \\vdots \\\\ \\sum_{i=1}^n x_{ip} w_i g_{\\mu}(\\mu_i) (y_i - \\mu_i) \\end{bmatrix} \\tag{3.11} \\end{align}\\] 여기서 가중치 \\(w_i\\)는 다음과 정의한다. 가중치 \\(w_i\\)는 앞에서 설명한 작업 변량의 분산의 역수와 동일하다. 식 (3.8) 을 참조하자. \\[ w_i \\equiv \\frac{1}{ g^2_\\mu(\\mu_i) a(\\phi_i) v(\\mu_i) } \\]. 3.3.2 가능도 방정식의 유도: 두 번째 방법 위의 방정식 (3.9) 에 미분의 연쇄법칙(chain rule)을 적용하면 다음과 같은 방정식을 얻는다. \\[\\begin{equation} \\pardiff{ \\ell_n }{ \\bm \\beta} = \\pardiff{ \\bm \\eta }{ \\bm \\beta } \\pardiff{ \\bm \\mu }{ \\bm \\eta } \\pardiff{ \\bm \\theta }{ \\bm \\mu } \\pardiff{ \\ell_n }{ \\bm \\theta } =0 \\tag{3.12} \\end{equation}\\] 위의 식에서 \\(\\bm \\eta\\), \\(\\bm \\mu\\), \\(\\bm \\theta\\)는 다음과 같이 \\(n\\)개의 대응되는 원소로 이루어진 벡터이다. \\[ \\bm \\eta = \\begin{bmatrix} \\eta_1 \\\\ \\eta_2 \\\\ \\vdots \\\\ \\eta_n \\end{bmatrix}, \\quad \\bm \\mu = \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_n \\end{bmatrix}, \\quad \\bm \\theta = \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix} \\] 이제 식 (3.12)에서 나타난 도함수들을 각각 구해보자. 먼저 \\[\\begin{align} \\pardiff{ \\bm \\eta }{ \\bm \\beta } &amp; = \\left [ \\pardiff{ \\eta_1 }{ \\bm \\beta } ~~ \\pardiff{ \\eta_2 }{ \\bm \\beta } ~~ \\dots ~~ \\pardiff{ \\eta_n }{ \\bm \\beta } \\right ] \\\\ &amp; = \\left [ \\pardiff{ {\\bm x}^t_1 \\bm \\beta }{ \\bm \\beta } ~~ \\pardiff{ {\\bm x}^t_2 \\bm \\beta }{ \\bm \\beta } ~~ \\dots ~~ \\pardiff{ {\\bm x}^t_n \\bm \\beta }{ \\bm \\beta } \\right ] \\\\ &amp; = [ {\\bm x}_1 ~~ {\\bm x}_2 ~~ \\dots ~~ {\\bm x}_n ] \\\\ &amp; = {\\bm X}^t \\end{align}\\] 위의 식에서 행렬 \\(\\bm X\\)는 \\(n \\times p\\) 계획행렬이다. 또한 \\[\\begin{align} \\pardiff{ \\bm \\mu }{ \\bm \\eta } &amp; = \\left [ \\pardiff{ \\mu_1 }{ \\bm \\eta } ~~ \\pardiff{ \\mu_2 }{ \\bm \\eta } ~~ \\dots ~~ \\pardiff{ \\mu_n }{ \\bm \\eta } \\right ] \\\\ &amp; = \\left [ \\pardiff{ g^{-1}(\\eta_1) }{ \\bm \\eta } ~~ \\pardiff{ g^{-1}(\\eta_2) }{ \\bm \\eta } ~~ \\dots ~~ \\pardiff{ g^{-1}(\\eta_2) }{ \\bm \\eta } \\right ] \\\\ &amp; = \\begin{bmatrix} \\frac{1}{g&#39;(\\mu_1)} &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\frac{1}{g&#39;(\\mu_2)} &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; \\frac{1}{g&#39;(\\mu_n)} \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} g^{-1}_{\\mu}(\\mu_1) &amp; &amp; &amp; \\\\ &amp; g^{-1}_{\\mu}(\\mu_2) &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; g^{-1}_{\\mu}(\\mu_n) \\end{bmatrix} \\end{align}\\] 위에서 \\(\\pardiffl{ \\bm \\mu }{ \\bm \\eta }\\)는 \\(n\\)-차원 대각행렬이며 \\(g_\\mu(\\mu) = g&#39;(\\mu)\\)로서 연결함수 \\(g\\)를 1차 미분한 함수이다. 또한 다음과 같은 결과를 얻는다. \\[\\begin{align} \\pardiff{ \\bm \\theta }{ \\bm \\mu } &amp; = \\left [ \\pardiff{ \\theta_1 }{ \\bm \\mu } ~~ \\pardiff{ \\theta_2 }{ \\bm \\mu } ~~ \\dots ~~ \\pardiff{ \\theta_n }{ \\bm \\mu } \\right ] \\\\ &amp; = \\begin{bmatrix} \\frac{1}{b&#39;&#39;(\\theta_1)} &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\frac{1}{b&#39;&#39;(\\theta_2)} &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; \\frac{1}{b&#39;&#39;(\\theta_n)} \\end{bmatrix} \\\\ &amp; = \\begin{bmatrix} v^{-1}(\\mu_1) &amp; &amp; &amp; \\\\ &amp; v^{-1}(\\mu_2) &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; v^{-1}(\\mu_n) \\end{bmatrix} \\end{align}\\] 마지막으로 식 (2.7) 과 (3.6) 를 이용하면 로그가능도함수 \\(\\ell_n\\) 을 \\(\\bm \\theta\\)로 미분한 \\(n\\)-차원 벡터는 다음과 같이 얻어진다. \\[\\begin{equation} \\pardiff{ \\ell_n }{ \\bm \\theta} = \\begin{bmatrix} \\frac{y_1 - b&#39;(\\theta_1)}{a(\\phi_1)} \\\\ \\frac{y_2 - b&#39;(\\theta_2)}{a(\\phi_2)} \\\\ \\vdots \\\\ \\frac{y_n - b&#39;(\\theta_n)}{a(\\phi_n)} \\end{bmatrix} = \\begin{bmatrix} \\frac{y_1 - \\mu_1}{a(\\phi_1)} \\\\ \\frac{y_2 - \\mu_2}{a(\\phi_2)} \\\\ \\vdots \\\\ \\frac{y_n - \\mu_n}{a(\\phi_n)} \\end{bmatrix} \\end{equation}\\] 이제 가능도추정을 위한 방정식 (3.12) 을 위에서 유도한 도함수 벡터와 행렬을 이용하여 다시 쓰면 다음과 같다. \\[\\begin{align} \\bm 0 &amp; = \\pardiff{ \\ell_n }{ \\bm \\beta} = \\pardiff{ \\bm \\eta }{ \\bm \\beta } \\pardiff{ \\bm \\mu }{ \\bm \\eta } \\pardiff{ \\bm \\theta }{ \\bm \\mu } \\pardiff{ \\ell }{ \\bm \\theta } \\\\ &amp; = {\\bm X}^t \\begin{bmatrix} \\frac{1}{g_{\\mu}(\\mu_1)} &amp; &amp; &amp; \\\\ &amp; \\frac{1}{g_{\\mu}(\\mu_1)} &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; \\frac{1}{g_{\\mu}(\\mu_n)} \\end{bmatrix} \\begin{bmatrix} \\frac{1}{v(\\mu_1)} &amp; &amp; &amp; \\\\ &amp; \\frac{1}{v(\\mu_2)} &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; \\frac{1}{v(\\mu_n)} \\end{bmatrix} \\begin{bmatrix} \\frac{y_1 - \\mu_1}{a(\\phi_1)} \\\\ \\frac{y_2 - \\mu_2}{a(\\phi_2)} \\\\ \\vdots \\\\ \\frac{y_n - \\mu_n}{a(\\phi_n)} \\end{bmatrix} \\\\ &amp;= {\\bm X}^t \\begin{bmatrix} \\frac{1}{g^2_{\\mu}(\\mu_1) a(\\phi_1) v(\\mu_1)} &amp; &amp; &amp; \\\\ &amp; \\frac{1}{g^2_{\\mu}(\\mu_1)a(\\phi_2)v(\\mu_2)} &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; \\frac{1}{g^2_{\\mu}a(\\phi_n)(\\mu_n)v(\\mu_n)} \\end{bmatrix} \\\\ &amp; \\quad \\quad \\times \\begin{bmatrix} g_{\\mu}(\\mu_1) &amp; &amp; &amp; \\\\ &amp; g_{\\mu}(\\mu_2) &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; g_{\\mu}(\\mu_n) \\end{bmatrix} \\begin{bmatrix} y_1 - \\mu_1 \\\\ y_2 - \\mu_2 \\\\ \\vdots \\\\ y_n - \\mu_n \\end{bmatrix} \\\\ &amp; = {\\bm X}^t \\bm W \\bm \\Delta (\\bm y - \\bm \\mu) \\tag{3.13} \\end{align}\\] 위의 식에서 가중값 대각행렬 \\(\\bm W\\), 연결함수 미분값 대각행렬 \\(\\bm \\Delta\\), 관측값 벡터 \\(\\bm y\\), 평균 벡터 \\(\\mu\\)는 다음과 같이 정의된다. \\[\\begin{align} \\bm W &amp; = \\begin{bmatrix} w_1 &amp; &amp; &amp; \\\\ &amp; w_2 &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; w_n \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{g^2_{\\mu}(\\mu_1) a(\\phi_1) v(\\mu_1)} &amp; &amp; &amp; \\\\ &amp; \\frac{1}{g^2_{\\mu}(\\mu_1)a(\\phi_2)v(\\mu_2)} &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; \\frac{1}{g^2_{\\mu}a(\\phi_n)(\\mu_n)v(\\mu_n)} \\end{bmatrix} \\\\ \\bm \\Delta &amp; = \\begin{bmatrix} \\delta_1 &amp; &amp; &amp; \\\\ &amp; \\delta_2 &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; \\delta_n \\end{bmatrix} = \\begin{bmatrix} g_{\\mu}(\\mu_1) &amp; &amp; &amp; \\\\ &amp; g_{\\mu}(\\mu_2) &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; g_{\\mu}(\\mu_n) \\end{bmatrix} \\\\ \\bm y &amp; = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, \\quad \\bm \\mu = \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_n \\end{bmatrix} \\end{align}\\] 3.4 최대가능도추정량의 계산 이제 회귀계수 \\(\\beta\\)를 최대가능도추정법(Maximum Likelihood Estimation)으로 가능도 방정식을 식 (3.13)를 이용하면 다음과 같은 행렬 방정식으로 표시된다. \\[\\begin{equation} {\\bm X}^t \\bm W \\bm \\Delta \\bm y = {\\bm X}^t \\bm W \\bm \\Delta \\bm \\mu \\tag{3.14} \\end{equation}\\] 위의 방정식은 일반적으로 회귀계수 벡터 \\(\\bm \\beta\\)에 대하여 선형방정식이 아니므로 최소제곱법과 같이 최대가능도 추정량을 직접 구할 수 없다. 정규분포 가정 하에서 선형회귀 모형에서는 식 (??)이 최소제곱법의 방정식 \\({\\bm X}^t \\bm y = {\\bm X}^t {\\bm X} \\bm \\beta\\)로 유도되고 직접적으로 구할 수 있다. 많은 경우 스케일 모수 \\(a(\\phi_i)\\)는 관측값 \\(y_i\\)에 따라 변하지 않고 상수인 경우가 흔하다. 즉 \\(a(\\phi_i) \\equiv a(phi)\\). 이러한 경우 가능도 방정식 (3.11) 또는 (3.13) 에서 스케일 모수 \\(a(\\phi_i)\\)를 1로 놓고 방정식을 푼다. 최대가능도추정량을 실제 계산하기 위하여 로그 가능도 함수의 2차 도함수(헤시안) 행렬을 구해보자. 식 (3.11) 에서 얻은 1차 도함수를 한번 더 미분하면 다음과 같은 결과를 얻는다. \\[\\begin{align*} \\pardiffdd{\\ell_n}{\\bm \\beta}{\\bm \\beta^t} &amp; = \\sum \\pardiff{}{\\bm \\beta} \\left [ {\\bm x}_i w_i g_\\mu(\\mu_i) (y_i - \\mu_i) \\right ] \\\\ &amp; = \\sum \\pardiff{}{\\bm \\beta} \\left [ {\\bm x}_i c_i (y_i - \\mu_i) \\right ] \\quad [ c_i \\equiv w_i g_\\mu(\\mu_i) ] \\\\ &amp; = \\sum \\left [ \\pardiff{ c_i (y_i - \\mu_i)}{\\bm \\beta} { \\bm x}^t_i \\right ] \\\\ &amp; = \\sum \\left [ \\pardiff{ c_i }{\\bm \\beta} (y_i - \\mu_i)+ \\pardiff{ (y_i - \\mu_i)}{\\bm \\beta} c_i \\right ] {\\bm x}^t_i \\\\ &amp; = \\sum \\left [ \\pardiff{ c_i }{\\bm \\beta} (y_i - \\mu_i) - \\pardiff{ \\eta_i}{\\bm \\beta} \\pardiff{ \\mu_i}{\\eta_i} c_i \\right ] {\\bm x}^t_i \\\\ &amp;= \\sum \\left [ \\pardiff{ c_i }{\\bm \\beta} (y_i - \\mu_i)- {\\bm x}_i [g_\\mu(\\mu)]^{-1} c_i \\right ] {\\bm x}^t_i \\quad [ c_i [g_\\mu(\\mu)]^{-1} = w_i] \\\\ &amp;= \\sum \\left [ \\pardiff{ c_i }{\\beta} (y_i - \\mu_i) \\right ] {\\bm x}^t_i - \\sum {\\bm x}_i w_i {\\bm x}^t_i \\\\ &amp;= \\sum \\left [ \\pardiff{ c_i }{\\beta} (y_i - \\mu_i) \\right ] {\\bm x}^t_i - {\\bm X}^t \\bm W {\\bm X} \\end{align*}\\] 그러므로 피셔정보 \\(\\bm I(\\bm \\beta)\\)는 다음과 같이 얻어진다. \\[\\begin{align} \\bm I(\\bm \\beta) &amp; = - E \\left [ \\pardiffdd{\\ell_n}{\\bm \\beta}{\\bm \\beta^t} \\right ] \\\\ &amp; = E \\left [ - \\sum \\left [ \\pardiff{ c_i }{\\beta} (y_i - \\mu_i) \\right ] {\\bm x}^t_i + {\\bm X}^t \\bm W {\\bm X} \\right ] \\\\ &amp; = \\bm 0+ {\\bm X}^t \\bm W {\\bm X} \\tag{3.15} \\end{align}\\] 또는 식 (3.13) 에서 얻은 1차 도함수 방정식을 를 한번 더 미분하여 기대값을 취하면 식 (3.15)와 동일한 결과를 얻는다. \\[\\begin{align} \\pardiffdd{\\ell_n}{\\bm \\beta}{\\bm \\beta^t} &amp; = \\pardiff{}{\\bm \\beta} \\left [ {\\bm X}^t \\bm W \\bm \\Delta (\\bm y - \\bm \\mu) \\right ] \\\\ &amp; = \\left \\{ \\pardiff{}{\\bm \\beta} \\left [ {\\bm X}^t \\bm W \\bm \\Delta \\right ] \\right \\} (\\bm y - \\bm \\mu) + \\left \\{ \\pardiff{}{\\bm \\beta} \\left [ (\\bm y - \\bm \\mu)^t \\right ] \\right \\} \\bm \\Delta \\bm W {\\bm X} \\\\ &amp; = \\left \\{ \\pardiff{}{\\bm \\beta} \\left [ {\\bm X}^t \\bm W \\bm \\Delta \\right ] \\right \\} (\\bm y - \\bm \\mu) -\\left [ \\pardiff{\\bm \\mu}{\\bm \\beta} \\right ] \\bm \\Delta \\bm W {\\bm X} \\\\ &amp; = \\left \\{ \\pardiff{}{\\bm \\beta} \\left [ {\\bm X}^t \\bm W \\bm \\Delta \\right ] \\right \\} (\\bm y - \\bm \\mu) -\\left [ \\pardiff{\\bm \\eta}{\\bm \\beta} \\pardiff{\\bm \\mu}{\\bm \\eta} \\right ] \\bm \\Delta \\bm W {\\bm X} \\\\ &amp; = \\left \\{ \\pardiff{}{\\bm \\beta} \\left [ {\\bm X}^t \\bm W \\bm \\Delta \\right ] \\right \\} (\\bm y - \\bm \\mu) -\\left [ {\\bm X}^t {\\bm \\Delta}^{-1} \\right ] \\bm \\Delta \\bm W {\\bm X} \\\\ &amp; = \\left \\{ \\pardiff{}{\\bm \\beta} \\left [ {\\bm X}^t \\bm W \\bm \\Delta \\right ] \\right \\} (\\bm y - \\bm \\mu) -{\\bm X}^t \\bm W {\\bm X} \\\\ \\end{align}\\] 최대가능도추정량 \\(\\hat {\\bm \\beta}\\)는 가능도 방정식 (3.14) 을 직접 풀어서 계산할 수 있지만 믾은 경우 직접해(explicit solution)를 구하는 것이 불가능 하다. 따라서 보통의 경우 선형화된 작업변량에 반복가중최소제곱법(iterative weighted least square; IWLS)을 적용하여 최대가능도 추정량을 구하며 IWLS로 구하는 해는 가능도 방정식 (3.14)의 해와 동일하다. 주어진 분포에서 기본 연결함수를 \\(g\\) 라고 하고 관측값 \\(y\\)를 변환한 작업변량 \\(z=g(y)\\)의 테일러 전개를 다음과 같이 고려해 보자. \\[ z \\equiv g(y) \\cong g(\\mu) + g_\\mu (\\mu) (y-\\mu) \\] 작업 변량 \\(z\\) 의 분산은 식 (3.8) 와 같이 다음으로 주어진다. \\[ var(z) = v(\\mu) g^2_\\mu (\\mu) \\equiv w^{-1} \\] 회귀계수 벡터의 초기값을 \\({\\bm \\beta}_0\\)라고 하자. 그러면 작업 변량의 초기값 \\(z_0\\)는 \\({\\bm \\beta}_0\\) 로 계산된 \\(\\mu_0\\)를 이용하여 다음과 같이 구할 수 있다. \\[ z_0 = g(\\mu_0) + g_\\mu (\\mu_0) (y-\\mu_0) = \\eta_0 + g_\\mu (\\mu_0) (y-\\mu_0) \\] IWLS 추정량 \\(\\hat {\\bm \\beta}\\)는 \\(z_0\\) 를 설명변수 벡터 \\(\\bm x\\)로 선형회귀분석을 적합할 때 가중치를 \\(w_0\\) 로 이용하는 가중최소제곱법으로 반복적으로 적용하여 개선할 수 있다. 실제로 IWLS 추정량은 피셔정보를 이용한 스코어 방법(Fisher scoring method)로 구한 최대가능도 추정량과 동일함을 보일 수 있다. 일단 회귀계수 벡터의 초기값 \\(\\hat {\\bm \\beta}^0\\) 으로 계산된 피셔정보 행렬을 \\(\\bm A\\)로 아래와 같이 정의하자. \\[ \\bm A = -E \\left [ \\pardiffdd{\\ell_n}{\\bm \\beta}{ {\\bm \\beta}^t} \\right ]_{\\bm \\beta= \\hat {\\bm \\beta}^0} \\] 새로운 추정량 \\(\\hat {\\bm \\beta}^1\\) 가 이전의 추정량 \\(\\hat {\\bm \\beta}^0\\)에서 다음과 같은 축차식으로 계산되는 방법이 피셔 스코링 방법이다. \\[ 0 = \\pardiff{\\ell_n }{\\bm \\beta} |_{\\bm \\beta=\\hat {\\bm \\beta}^0}-\\bm A(\\hat {\\bm \\beta}^1 - \\hat {\\bm \\beta}^0) \\quad \\Leftrightarrow \\quad \\bm A(\\hat {\\bm \\beta}^1 - \\hat {\\bm \\beta}^0) = \\pardiff{\\ell_n }{\\bm \\beta} |_{\\bm \\beta=\\hat {\\bm \\beta}^0} \\] 위의 피셔의 스코링 방법을 더 정리하면 다음과 같이 유도할 수 있다. \\[\\begin{align*} \\bm A( \\hat {\\bm \\beta}^1 - \\hat {\\bm \\beta}^0) &amp; = \\pardiff{ \\ell_n }{\\bm \\beta} |_{\\bm \\beta=\\hat {\\bm \\beta}^0} \\\\ \\Leftrightarrow {\\bm X}^t {\\bm W}_0 \\bm X (\\hat {\\bm \\beta}^1 - \\hat {\\bm \\beta}^0) &amp; = {\\bm X}^t {\\bm W}_0 {\\bm \\Delta}_0 (\\bm y-{\\bm \\mu}_0) \\\\ \\Leftrightarrow {\\bm X}^t {\\bm W}_0 \\bm X \\hat {\\bm \\beta}^1 &amp; = {\\bm X}^t {\\bm W}_0 [ \\bm X \\hat {\\bm \\beta}^0+ {\\bm \\Delta}_0 (\\bm y-{\\bm \\mu}_0)] \\\\ \\Leftrightarrow {\\bm X}^t {\\bm W}_0 \\bm X \\hat {\\bm \\beta}^1 &amp; = {\\bm X}^t {\\bm W}_0 [ {\\bm \\eta}_0+ {\\bm \\Delta}_0 (\\bm y-{\\bm \\mu}_0)] \\\\ \\Leftrightarrow {\\bm X}^t {\\bm W}_0 \\bm X \\hat {\\bm \\beta}^1 &amp; = {\\bm X}^t {\\bm W}_0 {\\bm z}_0 \\\\ \\end{align*}\\] 위의 방정식은 \\(z_0 = \\eta_0 + g_\\mu (\\mu_0) (y-\\mu_0)\\)를 가중치 \\(w_0\\)를 사용하여 얻은 가중최소제곱법에서 나온 방정식임을 알 수 있다. 따라서 최대가능도 추정량을 구하는 피셔의 스코링 방법은 앞에서 알아본 반복가중최소제곱법과 동일하다. 예제 3.1 (이항분포) 3.5 Maximum Quasi-Likelihood Each observation \\(y\\) is estimated by its the estimator\\(\\hat \\mu\\) and the goodness of fit should be assessed. We shall be concerned with the formed form the logarithm of a ratio of likelihoods, to be called deviance. We consider two extreme models: the null model and the full (saturated) model. The null model consider only one parameter for all y’s so that it is a simplest model. The full model assumes there are \\(n\\) parameters for \\(n\\) observations so that each observation contribute to one parameter only (perfect fit). Let \\(\\ell (y , \\theta, \\psi ) = \\ell ( \\theta, \\psi;y )\\) be the log likelihood function and we can describe it as a function of \\(\\mu\\) such as \\(\\ell ( \\theta, \\psi;y )= \\ell ( \\mu, \\psi;y )\\). The deviance is defined as \\[ D(y,\\mu,\\psi) = 2 \\{ \\ell ( \\hat \\mu_F, \\psi;y ) - \\ell ( \\hat \\mu, \\psi;y ) \\} \\] where \\(\\hat \\mu_F\\) is estimator under the full model and \\(\\hat \\mu\\) is the maximum likelihood estimator under the model considered. The scaled deviance is defined as \\[ D(y,\\mu) =D(y,\\mu,\\psi) a(\\psi) \\] References "],
["mulivar.html", "부록 A: 다변량 확률변수의 성질 A.1 일변량분포 A.2 확률벡터와 분포 A.3 다변량 정규분포 A.4 표준정규분포로의 변환 A.5 예제", " 부록 A: 다변량 확률변수의 성질 A.1 일변량분포 일변량 확률변수 \\(X\\)가 확률밀도함수 \\(f(x)\\)를 가지는 분포를 따를때 기대값과 분산은 다음과 같이 정의된다. \\[ E(X) = \\int x f(x) dx = \\mu, \\quad V(X) = E[ X-E(X)]^2=\\int (x-\\mu)^2 f(x) dx =\\sigma^2 \\] 새로운 확률변수 \\(Y\\)가 확률변수 \\(X\\)의 선형변환으로 표시된다면 (\\(a\\)와 \\(b\\)는 실수) \\[ Y = aX+b\\] 그 기대값(평균)과 분산은 다음과 같이 계산된다. \\[\\begin{eqnarray*} E(Y) &amp;=&amp; E(aX+b) \\\\ &amp;=&amp; \\int (ax+b) f(x) dx \\\\ &amp;=&amp; a \\int x f(x) dx + b \\\\ &amp;=&amp; a E(X) + b\\\\ &amp;=&amp; a \\mu + b \\\\ V(Y) &amp;=&amp; Var(aX+b) \\\\ &amp;=&amp; E[aX+b -E(aX+b)]^2 \\\\ &amp;=&amp; E[a(X-\\mu)]^2 \\\\ &amp;=&amp; a^2 E(X-\\mu)^2\\\\ &amp;=&amp; a^2 \\sigma^2 \\end{eqnarray*}\\] A.2 확률벡터와 분포 확률벡터 \\(\\bm X\\)가 \\(p\\) 차원의 다변량분포를 따른다고 하고 결합확률밀도함수 \\(f(\\bm x) =f(x_1,x_2,\\dots,x_p)\\)를 를 가진다고 하자. \\[\\begin{equation*} \\bm X = \\begin{pmatrix} X_1 \\\\ X_2 \\\\ X_3 \\\\ .. \\\\ X_p \\end{pmatrix} \\end{equation*}\\] 다변량 확률벡터의 기대값(평균벡터)과 공분산(행렬)은 다음과 같이 계산된다. \\[\\begin{equation*} \\bm E(\\bm X) = \\begin{pmatrix} E(X_1) \\\\ E(X_2) \\\\ E(X_3) \\\\ .. \\\\ E(X_p) \\end{pmatrix} = \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\\\ .. \\\\ \\mu_p \\end{pmatrix} =\\bm \\mu \\end{equation*}\\] \\[\\begin{equation*} V(\\bm X) =Cov(\\bm X) = E (\\bm X-\\bm \\mu) (\\bm X-\\bm \\mu)^t = \\begin{pmatrix} \\sigma_{11} &amp; \\sigma_{12} &amp; \\dots &amp; \\sigma_{1p} \\\\ \\sigma_{12} &amp; \\sigma_{22} &amp; \\dots &amp; \\sigma_{2p} \\\\ &amp; \\dots &amp; \\dots &amp; \\\\ \\sigma_{1p} &amp; \\sigma_{2p} &amp; \\dots &amp; \\sigma_{pp} \\\\ \\end{pmatrix} = \\bm \\Sigma \\end{equation*}\\] 여기서 \\(\\sigma_{ii}=V(X_i)\\), \\(\\sigma_{ij} = Cov(X_i, X_j)=Cov(X_j, X_i)\\)이다. 따라서 공분산 행렬 \\(\\bm \\Sigma\\)는 대칭행렬(symmetric matrix)이다. 다음 공식은 유용한 공식이다. \\[ \\bm \\Sigma = E (\\bm X-\\bm \\mu) (\\bm X-\\bm \\mu)^t = E(\\bm X \\bm X^t)-\\bm \\mu \\bm \\mu^t \\] 두 확률변수의 상관계수 \\(\\rho_{ij}\\)는 다음과 같이 정의된다. \\[ \\rho_{ij} = \\frac{Cov(X_i, X_j)}{ \\sqrt{V(X_i) V(X_j)}} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii} \\sigma_{jj}}} \\] 새로운 확률벡터 \\(\\bm Y\\)가 확률벡터 \\(\\bm X\\) 의 선형변환라고 하자. \\[ \\bm Y = \\bm A \\bm X + \\bm b \\] 단 여기서 \\(\\bm A = \\{ a_{ij} \\}\\)는 \\(p \\times p\\) 실수 행렬이고 \\(\\bm b =(b_1 b_2 \\dots b_p)^t\\)는 \\(p \\times 1\\) 실수 벡터이다. 확률벡터 \\(\\bm Y\\)의 기대값(평균벡터)과 공분산은 다음과 같이 계산된다. \\[\\begin{eqnarray*} E(\\bm Y ) &amp;=&amp; E(\\bm A \\bm X+ \\bm b) \\\\ &amp;=&amp; \\bm A E(\\bm X)+ \\bm b \\\\ &amp;=&amp; \\bm A \\bm \\mu+ \\bm b \\\\ V(\\bm Y) &amp;=&amp; Var(\\bm A \\bm X+ \\bm b) \\\\ &amp;=&amp; E[\\bm A \\bm X+ \\bm b -E(\\bm A \\bm X+ \\bm b)] [\\bm A \\bm X+ \\bm b -E(\\bm A \\bm X+ \\bm b)]^t \\\\ &amp;=&amp; E[\\bm A \\bm X - \\bm A \\bm \\mu] [\\bm A \\bm X - \\bm A \\bm \\mu]^t \\\\ &amp;=&amp; E[\\bm A (\\bm X - \\bm \\mu)] [\\bm A (\\bm X - \\bm \\mu)]^t \\\\ &amp;=&amp; \\bm A E [(\\bm X - \\bm \\mu) (\\bm X - \\bm \\mu)^t] \\bm A^t \\\\ &amp;=&amp; \\bm A \\bm \\Sigma \\bm A^t \\end{eqnarray*}\\] 만약 표본 \\(\\bm X_i, \\bm X_2, \\dots, \\bm X_n\\) 이 독립적으로 평균이 \\(\\bm \\mu\\) 이고 공분산이 \\(\\bm \\Sigma\\) 인 분포에서 추출되었다면 표본의 평균벡터 \\(\\bar {\\bm X}\\) 는 평균이 \\(\\bm \\mu\\) 이고 공분산이 \\(\\frac{1}{n}\\bm \\Sigma\\) 인 분포를 따른다. \\[\\begin{equation*} \\bar {\\bm X} = \\begin{pmatrix} \\sum_{i=1}^n X_{i1} / n \\\\ \\sum_{i=1}^n X_{i2} / n \\\\ \\sum_{i=1}^n X_{i3} / n \\\\ .. \\\\ \\sum_{i=1}^n X_{ip} / n \\end{pmatrix} \\end{equation*}\\] 여기서 \\(X_{ij}\\) 는 \\(i\\)번째 표본벡터 \\(\\bm X_i =(X_{i1} X_{i2} \\dots X_{ip})^t\\)의 \\(j\\)번째 확률변수이다. A.3 다변량 정규분포 일변량 확률변수 \\(X\\)가 평균이 \\(\\mu\\) 이고 분산이 \\(\\sigma^2\\)인 정규분포를 따른다면 다음과 같이 나타내고 \\[ X \\sim N(\\mu, \\sigma^2 ) \\] 확률밀도함수 \\(f(x)\\) 는 다음과 갇이 주어진다. \\[ f(x) = (2 \\pi \\sigma^2)^{-1/2} \\exp \\left ( - \\frac{(x-\\mu)^2}{2} \\right ) \\] \\(p\\)-차원 확률벡터 \\(\\bm X\\)가 평균이 \\(\\bm \\mu\\) 이고 공분산이 \\(\\bm \\Sigma\\)인 다변량 정규분포를 따른다면 다음과 같이 나타내고 \\[ \\bm X \\sim N_p(\\bm \\mu, \\bm \\Sigma ) \\] 확률밀도함수 \\(f(\\bm x)\\) 는 다음과 갇이 주어진다. \\[ f(\\bm x) = (2 \\pi)^{-p/2} | \\bm \\Sigma|^{-1/2} \\exp \\left ( - \\frac{(\\bm x-\\bm \\mu) \\bm \\Sigma^{-1}(\\bm x-\\bm \\mu)^t}{2} \\right ) \\] 다변량 정규분포 \\(N(\\bm \\mu, \\bm \\Sigma)\\)를 따르는 확률벡터 \\(\\bm X\\)를 다음과 같이 두 부분으로 나누면 \\[ \\bm X = \\begin{bmatrix} \\bm X_1 \\\\ \\bm X_2 \\end{bmatrix}, \\quad \\bm X_1 = \\begin{bmatrix} \\bm X_{11} \\\\ \\bm X_{12} \\\\ \\bm \\vdots \\\\ \\bm X_{1p} \\end{bmatrix}, \\quad \\bm X_2= \\begin{bmatrix} \\bm X_{21} \\\\ \\bm X_{22} \\\\ \\bm \\vdots \\\\ \\bm X_{2q} \\end{bmatrix} \\] 각각 다변량 정규분포를 따르고 다음과 같이 나타낼 수 있다. \\[ \\begin{bmatrix} E(\\bm X_1) \\\\ E(\\bm X_2) \\end{bmatrix} = \\begin{bmatrix} \\bm \\mu_1 \\\\ \\bm \\mu_2 \\end{bmatrix} , \\quad \\begin{bmatrix} V(\\bm X_1) &amp; Cov(\\bm X_1, X_2) \\\\ Cov(\\bm X_2 X_1) &amp; V(\\bm X_2) \\end{bmatrix} = \\begin{bmatrix} \\bm \\Sigma_{11} &amp; \\bm \\Sigma_{12} \\\\ \\bm \\Sigma^t_{12} &amp; \\bm \\Sigma_{22} \\end{bmatrix} \\] \\[ \\bm X = \\begin{bmatrix} \\bm X_1 \\\\ \\bm X_2 \\end{bmatrix} \\sim N_{p+q} \\left ( \\begin{bmatrix} \\bm \\mu_1 \\\\ \\bm \\mu_2 \\end{bmatrix} ,\\begin{bmatrix} \\bm \\Sigma_{11} &amp; \\Sigma_{12} \\\\ \\bm \\Sigma^t_{12} &amp; \\Sigma_{22} \\end{bmatrix} \\right ) \\] 확률벡터 \\(\\bm X_2 = \\bm x_2\\)가 주어진 경우 \\(\\bm X_1\\)의 조건부 분포는 \\(p\\)-차원 다변량 정규분포를 따르고 평균과 공분산은 다음과 같다. \\[ E(\\bm X_1 | \\bm X_2 = \\bm x_2 ) = \\bm \\mu_1 + \\bm \\Sigma_{12} \\bm \\Sigma^{-1}_{22} (\\bm \\mu_2 - \\bm x_2), \\quad V(\\bm X_1 | \\bm X_2 = \\bm x_2 ) = \\bm \\Sigma_{11} -\\bm \\Sigma_{12} \\bm \\Sigma^{-1}_{22} \\bm \\Sigma^t_{12} \\] 예를 들어 \\(2\\)-차원 확률벡터 \\(\\bm X=(X_1, X_2)^t\\)가 평균이 \\(\\bm \\mu=(\\mu_1,\\mu_2)^t\\) 이고 공분산 \\(\\bm \\Sigma\\)가 다음과 같이 주어진 \\[\\begin{equation*} \\bm \\Sigma = \\begin{pmatrix} \\sigma_{11} &amp; \\sigma_{12} \\\\ \\sigma_{12} &amp; \\sigma_{22} \\end{pmatrix} \\end{equation*}\\] 이변량 정규분포를 따른다면 확률밀도함수 \\(f(\\bm x)\\)에서 \\(\\exp\\)함수의 인자는 다음과 같이 주어진다. \\[\\begin{eqnarray*} &amp;(\\bm x-\\bm \\mu) \\bm \\Sigma^{-1}(\\bm x-\\bm \\mu)^t = \\\\ &amp;-\\frac{1}{2 (1-\\rho^2)} \\left [ \\left ( \\frac{(x_1-\\mu_1)^2}{\\sigma_{11}} \\right ) +\\left ( \\frac{(x_2-\\mu_2)^2}{\\sigma_{22}} \\right ) -2 \\rho \\left ( \\frac{(x_1-\\mu_1)}{\\sqrt{\\sigma_{11}}} \\right ) \\left ( \\frac{(x_2-\\mu_2)}{\\sqrt{\\sigma_{22}}} \\right ) \\right ] \\end{eqnarray*}\\] 그리고 \\(p=2\\)인 경우 확률밀도함수의 상수부분은 다음과 같이 주어진다. \\[ (2 \\pi)^{-p/2} | \\bm \\Sigma|^{-1/2} = \\frac{1}{ 2 \\pi \\sqrt{\\sigma_{11} \\sigma_{22} (1-\\rho^2)}} \\] 여기서 \\(\\rho = \\sigma_{12} / \\sqrt{\\sigma_{11} \\sigma_{22}}\\) 만약 \\(X_2 = x_2\\)가 주어졌을 때 \\(X_1\\)의 조건부 분포는 정규분포이고 평균과 분산은 다음과 같이 주어진다. \\[ E( X_1 | X_2 = x_2 ) = \\mu_1 + \\frac{\\sigma_{12}}{\\sigma_{22}} ( \\mu_2 - x_2) = \\mu_1 + \\rho \\frac{\\sqrt{\\sigma_{11}}}{\\sqrt{\\sigma_{22}}} ( \\mu_2 - x_2) \\] \\[ V( X_1 | X_2 = x_2 ) = \\sigma_{11} - \\frac{\\sigma^2_{12}}{\\sigma_{22}} = \\sigma_{11}(1-\\rho^2) \\] 다변량 정규분포에서 공분산이 0인 두 확률 변수는 독립이다. \\[ \\sigma_{ij} = 0 \\leftrightarrow X_i \\text{ and } X_j \\text{ are independent} \\] A.4 표준정규분포로의 변환 일변량 확률변수 \\(X\\)가 평균이 \\(\\mu\\) 이고 분산이 \\(\\sigma^2\\)인 경우 다음과 같은 선형변환을 고려하면. \\[ Z = \\frac{X - \\mu}{\\sigma} = (\\sigma^2)^{-1/2} (X-\\mu) \\] 확률변수 \\(Z\\) 는 평균이 \\(0\\) 이고 분산이 \\(1\\)인 분포를 따른다. \\(p\\)차원 확률벡터 \\(\\bm X\\) 가 평균이 \\(\\bm \\mu\\) 이고 공분산이 \\(\\bm \\Sigma\\)인 분포를 가진다고 가정하자. 공분산 행렬 \\(\\bm \\Sigma\\)는 양정치 행렬(positive definite matrix)이며 다음과 같은 행렬의 분해가 가능하다. \\[ \\Sigma = \\bm C \\bm C^t \\] 여기서 \\(\\bm C\\) 는 정칙행렬이며 역행렬 \\(\\bm C^{-1}\\)가 존재한다. 위와 같은 행렬의 분해는 스펙트럴 분해(spectral decomposition)을 이용하여 구할 수 있다. 공분산 행렬 \\(\\bm \\Sigma\\)는 양정치 행렬이므로 고유치(eigen value) \\((\\lambda_1, \\lambda_2,\\dots, \\lambda_p)\\)가 모두 양수이고 정규직교 고유벡터(orthonormal eigen vector)의 행렬 \\(\\bm P\\)을 이용하여 다음과 같은 분해가 가능하다. \\[ \\Sigma = \\bm P \\bm \\Lambda \\bm P^t = \\bm P \\bm \\Lambda^{1/2} \\Lambda^{1/2} \\bm P^t \\] 여기서 \\(\\bm \\Lambda\\)는 고유치 \\((\\lambda_1, \\lambda_2,\\dots, \\lambda_p)\\)를 대각원소로 가지는 대각행렬이며 \\(\\bm \\Lambda^{1/2}\\)는 고유치의 제곱근을 대각원소로 가지는 대각행렬이다. 따라서 \\(\\bm C = \\bm P \\bm \\Lambda^{1/2}\\)로 하면 위와 같은 행렬의 분해가 가능하다. 정규직교 고유벡터(orthonormal eigen vector)의 행렬 \\(\\bm P\\)는 직교행렬이므로 \\[ \\bm C^{-1} = (\\bm P \\bm \\Lambda^{1/2})^{-1} = \\bm \\Lambda^{-1/2} \\bm P^t \\] \\(p\\)차원 확률벡터 \\(\\bm X\\)의 다음과 같은 선형변환을 고려하면. \\[ \\bm Z = \\bm C^{-1} ( \\bm X- \\bm \\mu) = \\bm \\Lambda^{-1/2} \\bm P^t ( \\bm X- \\bm \\mu) \\] 확률벡터 \\(\\bm Z\\) 는 평균이 \\(\\bm 0\\) 이고 공분산이 \\(\\bm I\\)인 분포를 따른다 (why?). 확률벡터 \\(\\bm X\\)가 정규분포를 따른다면 선형변환한 확률벡터 \\(\\bm Z\\)도 정규분포를 따른다. A.5 예제 예를 들어 이변량확률벡터 \\(\\bm X\\)가 다음과 같은 평균벡터와 공분산을 가진 정규분포를 따른다고 하자 \\[\\begin{equation*} \\bm \\mu = \\begin{pmatrix} 1\\\\ 2 \\end{pmatrix} \\quad \\bm \\Sigma = \\begin{pmatrix} 2 &amp; 1\\\\ 1 &amp; 2 \\end{pmatrix} \\end{equation*}\\] 공분산행렬 \\(\\bm \\sigma\\)의 고유치는 \\(|\\bm \\sigma -\\lambda \\bm I|=0\\)의 방정식을 풀어 구할 수 있다. \\[\\begin{equation*} |\\bm \\sigma -\\lambda \\bm I| = \\begin{pmatrix} 2-\\lambda &amp; 1\\\\ 1 &amp; 2-\\lambda \\end{pmatrix} = \\lambda^2 -4 \\lambda +3=0 \\end{equation*}\\] 방정식을 풀면 고유치는 \\((\\lambda_1, \\lambda_2) = (3,1)\\)이다. 각 고유치에 대한 고유벡터 \\(\\bm p=(p_1, p_2)^t\\)는 \\(\\bm \\Sigma \\bm p = \\lambda \\bm p\\) 으로 구할 수 있다. 각 고유치에 대하여 방정식을 구하면 다음 두 개의 방정식을 얻을 수 있다. \\[\\begin{equation*} p_1 - p_2 = 1 \\text{ and } p_1 + p_2 = 0 \\end{equation*}\\] 정규직교 벡터의 조건을 만족 시키기 위해서 \\(p^2_1 + p^2_2=1\\)의 조건을 적용하면 다음과 같은 정규직교 고유행렬을 얻을 수 있다. \\[\\begin{equation*} \\bm P = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} &amp; -\\frac{1}{\\sqrt{2}}\\\\ \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\end{equation*}\\] 또한 \\[\\begin{equation*} \\bm \\Lambda = \\begin{pmatrix} 3 &amp; 0\\\\ 0 &amp; 1 \\end{pmatrix} \\quad \\bm \\Lambda^{1/2} = \\begin{pmatrix} \\sqrt{3} &amp; 0\\\\ 0 &amp; 1 \\end{pmatrix} \\end{equation*}\\] 따라서 \\(C^{-1} = \\Lambda^{-1/2} \\bm P^t\\) 이며 \\[\\begin{equation*} \\bm C^{-1} = \\bm \\Lambda^{-1/2} \\bm P^t = \\begin{pmatrix} \\frac{1}{\\sqrt{3}} &amp; 0\\\\ 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}}\\\\ -\\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{6}} &amp; \\frac{1}{\\sqrt{6}}\\\\ -\\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\end{equation*}\\] 위의 계산을 R 프로그램으로 다음과 같이 구현할 수 있다. mu &lt;- c(1,2) S &lt;- matrix(c(2,1,1,2),2,2) res&lt;- eigen(S) res ## eigen() decomposition ## $values ## [1] 3 1 ## ## $vectors ## [,1] [,2] ## [1,] 0.7071 -0.7071 ## [2,] 0.7071 0.7071 L &lt;- res$values P &lt;- res$vectors Lsqrt &lt;- diag(sqrt(L)) C &lt;- P %*% Lsqrt C ## [,1] [,2] ## [1,] 1.225 -0.7071 ## [2,] 1.225 0.7071 Cinv &lt;- solve(C) Cinv ## [,1] [,2] ## [1,] 0.4082 0.4082 ## [2,] -0.7071 0.7071 Cinv %*% S %*% t(Cinv) ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 "],
["vectordiff.html", "부록 B: 벡터미분 B.1 스칼라미분 B.2 벡터미분의 표기 방법 B.3 핵심공식 B.4 합성함수에 대한 미분공식", " 부록 B: 벡터미분 B.1 스칼라미분 벡터미분(Vector diffrential) 또는 행렬미분(Matrix differential)은 벡터와 행렬의 미분식에 대한 표기법을 정의하는 방법이다. 보통 스칼라(scalar)에 대한 미분은 일분수 함수 \\(f: \\Re^1 \\rightarrow \\Re^1\\)또는 다변수 함수(function of several variables) \\(f: \\Re^p \\rightarrow \\Re^1\\)에서 쉽게 정의된다. 만약 \\(y = f(x)\\) 또는 \\(y=f(\\bm x)\\)라고 하면 다음과 같이 미분이 주어진다. \\[ \\pardiff{y}{x}= \\pardiff{f(x)}{x} = f&#39;(x) \\] \\[ \\pardiff{y}{\\bm x} =\\pardiff{f(\\bm x)}{\\bm x} = \\left (\\pardiff{f(\\bm x)}{x_1},~\\pardiff{f(\\bm x)}{x_2},~\\cdots, \\pardiff{f(\\bm x)}{x_p} \\right ) = \\nabla f(x)\\] 함수가 다변수함수일 경우 함수의 값을 각 축의 변수로 미분한 것(partial derivative)을 벡터로 표시하는 것을 gradient 라고 한다. B.2 벡터미분의 표기 방법 이제 다변량함수(multivariate function), \\(f: \\Re^p \\rightarrow \\Re^q\\)에 대한 미분을 생각해보자. 앞 절에서 본것과 같이 스칼라 함수를 여러 변수로 미분하여 partial derivative를 구한 뒤 gradient를 만드는 경우 열벡터와 행벡터 중 하나를 선택해야 한다. 이러한 선택은 절대적인 것이 아니며 각 분야의 특성과 편의에 따라 다르게 선택 될 수 있다. 이제 간단한 예제를 고려해 보자. 두 열벡터 \\(\\bm x=(x_1,x_2)^t \\in \\Re_2\\), \\(\\bm y=(y_1,y_2,y_3)^t \\in \\Re^3\\)를 고려하고 다음과 같은 함수로 두 벡터의 관계가 정의된다고 하자. \\[ y_1 = x_1^2 + x_2, \\quad y_2= \\exp (x_1) + 3 x_2, \\quad y_3 = \\sin(x_1) + x_2^3 \\] 일단 각각의 partial derivative \\(\\pardiffl{y_i}{x_j}\\)를 구해야 하며 이는 scalar 미분으로 쉽게 구해진다. \\[\\begin{align*} \\pardiff{ y_1}{ x_1} &amp; = 2x_1, &amp; \\quad \\pardiff{ y_2}{ x_1} &amp; = \\exp(x_1), &amp; \\quad \\pardiff{ y_3}{ x_1} &amp; = \\cos(x_1) \\\\ \\pardiff{ y_1}{ x_2} &amp; = 1, &amp; \\quad \\pardiff{ y_2}{ x_1} &amp; = 3, &amp; \\quad \\pardiff{ y_3}{ x_1} &amp; = 3 x_2^2 \\\\ \\end{align*}\\] 통계학에서는 벡터 \\(\\bm y\\)를 벡터 \\(\\bm x\\)로 미분하려면 다음과 같이 분모 표기법 (Denominator layout)을 사용하여 표기한다. \\[\\begin{equation*} \\pardiff{ \\bm y}{\\bm x} \\equiv \\pardiff{ \\bm y^t}{\\bm x} \\underset{def}{\\equiv} \\begin{bmatrix} \\pardiff{ y_1}{ x_1} &amp; \\pardiff{ y_2}{ x_1} &amp; \\pardiff{ y_3}{ x_1} \\\\ \\pardiff{ y_1}{ x_2} &amp; \\pardiff{ y_2}{ x_2} &amp; \\pardiff{ y_3}{ x_2} \\\\ \\end{bmatrix} = \\begin{bmatrix} 2x_1 &amp; \\exp(x_1) &amp; \\cos(x_1) \\\\ 1 &amp; 3 &amp; 3x_2^2 \\end{bmatrix} \\end{equation*}\\] 즉 분모표기법은 분모를 열벡터로, 분자를 행벡터로 보고 각각 위치에 있는 변수들에 대하여 미분을 표기하는 방법이다. B.3 핵심공식 디음은 분모표기법을 이용한 가장 기본적이고 핵심적인 미분 공식들이다. 공식을 유도하는 경우 분모표기법에서는 \\(\\pardiffl{\\bm y}{\\bm x} \\equiv \\pardiffl{\\bm y^t}{\\bm x}\\) 임을 이용한다. 변환이 있거나 여러가지 곱이 있는 경우 미분할 대상 벡터를 가장 왼쪽에 전치형태(즉, 행벡터의 형태로)로 놓는 것이 필요하다. 예를 들어 \\[ \\pardiff{ \\bm a^t \\bm V \\bm f(\\bm x)}{\\bm x} = \\pardiff{\\bm f(\\bm x)^t \\bm V^t \\bm a}{\\bm x} = \\pardiff{\\bm f(\\bm x)^t }{\\bm x} \\bm V^t \\bm a = \\pardiff{\\bm f(\\bm x) }{\\bm x} \\bm V^t \\bm a \\] 또한 행렬은 교환법칙이 성립하지 않기 때문에 연산의 순서를 유지해야 하는 것을 유념하자. B.3.1 기본행렬 미분 벡터 \\(\\bm c\\)를 상수벡터하고 하자. \\[ \\pardiff{\\bm c}{ \\bm x} = \\bm 0, \\quad \\pardiff{\\bm x}{ \\bm x} = \\bm I \\] B.3.2 벡터-스칼라 미분 이 경우는 \\(\\bm x \\in \\Re^1\\), \\(\\bm y \\in \\Re^q\\) 인 경우이며 결과는 다음과 같이 행벡터로 결과가 주어진다. \\[ \\pardiff{ \\bm y}{ x} \\underset{def}{\\equiv} \\pardiff{ \\bm y^t}{ x} = \\left [ \\pardiff{ y_1}{ x}, ~ \\pardiff{ y_2}{ x},~ \\cdots, \\pardiff{ y_q}{ x} \\right ] \\] B.3.3 스칼라-벡터 미분 이 경우는 \\(\\bm x \\in \\Re^p\\), \\(\\bm y \\in \\Re^1\\) 인 경우이며 결과는 다음과 같이 열벡터로 결과가 주어진다. \\[ \\pardiff{ y}{\\bm x} = \\begin{bmatrix} \\pardiff{ y}{ x_1} \\\\ \\pardiff{ y}{ x_2} \\\\ \\vdots \\\\ \\pardiff{ y}{ x_p} \\end{bmatrix} \\] B.3.4 상수벡터와 내적에 대한 미분 열벡터 \\(\\bm a\\)를 \\(p \\times 1\\) 상수벡터이라고 하고 \\(y = \\bm a^t \\bm x = \\bm x^t \\bm a\\)라 하자. \\[ \\pardiff{ y}{\\bm x} = \\pardiff{ \\bm a^t \\bm x}{\\bm x} =\\pardiff{\\bm x^t \\bm a}{\\bm x} = \\begin{bmatrix} \\pardiff{ \\bm a^t \\bm x}{ x_1} \\\\ \\pardiff{ \\bm a^t \\bm x}{ x_2} \\\\ \\vdots \\\\ \\pardiff{ \\bm a^t \\bm x}{ x_p} \\end{bmatrix} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_p \\end{bmatrix} = \\bm a \\] B.3.5 선형변환에 대한 미분 행렬 \\(\\bm A\\)를 \\(q \\times p\\) 행렬이라고 하고 \\(\\bm y = \\bm A \\bm x\\)라 하자. 여기서 행렬 \\(\\bm A\\)를 다음과 같이 나타내자. \\[ \\bm A = \\begin{bmatrix} \\bm a_1^t \\\\ \\bm a_2^t \\\\ \\vdots \\\\ \\bm a_q^t \\\\ \\end{bmatrix} \\text{ or } \\bm A^t = [ \\bm a_1~ \\bm a_2~\\cdots~\\bm a_q] \\] 위의 내적에 대한 미분 결과를 이용하면 다음은 결과를 얻는다. \\[\\begin{align*} \\pardiff{ \\bm y}{\\bm x} &amp; = \\pardiff{ \\bm A \\bm x}{\\bm x} \\\\ &amp; \\equiv \\pardiff{ \\bm x^t \\bm A^t }{\\bm x} \\\\ &amp; = \\pardiff{ }{\\bm x} [ \\bm x^t \\bm a_1 ~ \\bm x^t \\bm a_2 ~ \\cdots ~\\bm x^t \\bm a_q ] \\\\ &amp; = [ \\pardiff{\\bm x^t \\bm a_1 }{\\bm x} ~ \\pardiff{\\bm x^t \\bm a_2 }{\\bm x} ~ \\cdots ~\\pardiff{\\bm x^t \\bm a_q }{\\bm x} ] \\\\ &amp; = [ \\bm a_1 ~ \\bm a_2 ~ \\cdots ~ \\bm a_q ] \\\\ &amp; = \\bm A^t \\end{align*}\\] 위의 결과를 응용하면 다음의 결과를 얻는다. \\[ \\pardiff{ \\bm A \\bm x}{\\bm x} = \\bm A^t \\quad \\text{ and } \\quad \\pardiff{ \\bm x^t \\bm A}{\\bm x} = \\bm A \\] B.3.5.1 이차형식 \\[ \\pardiff{\\bm x^t \\bm A \\bm x}{\\bm x} =\\pardiff{\\bm x^t}{\\bm x} \\bm A \\bm x + \\pardiff{ \\bm x^t \\bm A^t }{\\bm x} \\bm x = \\bm A \\bm x + \\bm A^t \\bm x \\] 만약 행렬 \\(\\bm A\\)가 대칭이면 \\[ \\pardiff{\\bm x^t \\bm A \\bm x}{\\bm x} = 2 \\bm A \\bm x \\] B.4 합성함수에 대한 미분공식 두 개의 다변량 함수를 고려하고 \\[ g: \\Re^p \\rightarrow \\Re^q, \\quad f:\\Re^q \\rightarrow \\Re^r \\] \\[\\begin{align*} \\pardiff{ \\bm f(\\bm g(\\bm x)) }{\\bm x} &amp; = \\pardiff{ \\bm f^t(\\bm g(\\bm x)) }{\\bm x} \\\\ &amp; = \\left [ \\pardiff{ f_1(\\bm g(\\bm x)) }{\\bm x},~ \\pardiff{ f_2(\\bm g(\\bm x)) }{\\bm x},~ \\cdots ~, \\pardiff{ f_r(\\bm g(\\bm x)) }{\\bm x} \\right ] \\\\ &amp; = \\begin{bmatrix} \\pardiff{ f_1(\\bm g(\\bm x)) }{x_1} &amp; \\pardiff{ f_2(\\bm g(\\bm x)) }{x_1} &amp; \\cdots &amp; \\pardiff{ f_r(\\bm g(\\bm x)) }{x_1} \\\\ \\pardiff{ f_1(\\bm g(\\bm x)) }{x_2} &amp; \\pardiff{ f_2(\\bm g(\\bm x)) }{x_2} &amp; \\cdots &amp; \\pardiff{ f_r(\\bm g(\\bm x)) }{x_2} \\\\ &amp; &amp; \\vdots &amp; \\\\ \\pardiff{ f_1(\\bm g(\\bm x)) }{x_p} &amp; \\pardiff{ f_2(\\bm g(\\bm x)) }{x_p} &amp; \\cdots &amp; \\pardiff{ f_r(\\bm g(\\bm x)) }{x_p} \\\\ \\end{bmatrix} \\\\ &amp; =\\begin{bmatrix} \\sum_{k=1}^q \\pardiff{ f_1 }{ g_k} \\pardiff{ g_k }{ x_1} &amp; \\sum_{k=1}^q \\pardiff{ f_2 }{ g_k} \\pardiff{ g_k }{x_1} &amp; \\cdots &amp; \\sum_{k=1}^q \\pardiff{ f_r }{ g_k} \\pardiff{ g_k }{x_1} \\\\ \\sum_{k=1}^q \\pardiff{ f_1 }{ g_k} \\pardiff{ g_k }{ x_2} &amp; \\sum_{k=1}^q \\pardiff{ f_2 }{ g_k} \\pardiff{ g_k }{x_2} &amp; \\cdots &amp; \\sum_{k=1}^q \\pardiff{ f_r }{ g_k} \\pardiff{ g_k }{x_2} \\\\ &amp; &amp; \\vdots &amp; \\\\ \\sum_{k=1}^q \\pardiff{ f_1 }{ g_k} \\pardiff{ g_k }{ x_p} &amp; \\sum_{k=1}^q \\pardiff{ f_2 }{ g_k} \\pardiff{ g_k }{x_p} &amp; \\cdots &amp; \\sum_{k=1}^q \\pardiff{ f_r }{ g_k} \\pardiff{ g_k }{x_p} \\\\ \\end{bmatrix} \\\\ &amp; = \\sum_{k=1}^q \\begin{bmatrix} \\pardiff{ f_1 }{ g_k} \\pardiff{ g_k }{ x_1} &amp; \\pardiff{ f_2 }{ g_k} \\pardiff{ g_k }{x_1} &amp; \\cdots &amp; \\pardiff{ f_r }{ g_k} \\pardiff{ g_k }{x_1} \\\\ \\pardiff{ f_1 }{ g_k} \\pardiff{ g_k }{ x_2} &amp; \\pardiff{ f_2 }{ g_k} \\pardiff{ g_k }{x_2} &amp; \\cdots &amp; \\pardiff{ f_r }{ g_k} \\pardiff{ g_k }{x_2} \\\\ &amp; &amp; \\vdots &amp; \\\\ \\pardiff{ f_1 }{ g_k} \\pardiff{ g_k }{ x_p} &amp; \\pardiff{ f_2 }{ g_k} \\pardiff{ g_k }{x_p} &amp; \\cdots &amp; \\pardiff{ f_r }{ g_k} \\pardiff{ g_k }{x_p} \\\\ \\end{bmatrix} \\\\ &amp; = \\sum_{k=1}^q \\begin{bmatrix} \\pardiff{ g_k }{ x_1} \\\\ \\pardiff{ g_k }{ x_2} \\\\ \\vdots \\\\ \\pardiff{ g_k }{ x_p} \\end{bmatrix} \\begin{bmatrix} \\pardiff{ f_1 }{ g_k} &amp; \\pardiff{ f_2 }{ g_k} &amp; \\cdots &amp; \\pardiff{ f_r }{ g_k} \\end{bmatrix} \\\\ &amp; = \\sum_{k=1}^q \\pardiff{ g_k }{\\bm x} \\pardiff{ \\bm f }{ g_k} \\\\ &amp; = \\left [ \\pardiff{ g_1 }{\\bm x} \\pardiff{ g_2 }{\\bm x} \\cdots \\pardiff{ g_q }{\\bm x} \\right ] \\begin{bmatrix} \\pardiffl{ \\bm f }{ g_1} \\\\ \\pardiffl{ \\bm f }{ g_2} \\\\ \\vdots \\\\ \\pardiffl{ \\bm f }{ g_q} \\end{bmatrix} \\\\ &amp;= \\pardiff{\\bm g} { \\bm x} \\pardiff{\\bm f} { \\bm g} \\quad (p \\times q)(q \\times r) \\end{align*}\\] 특별히 \\(f\\)가 일변량인 경우(\\(r=1\\)), \\[ \\pardiff{ f(\\bm g(\\bm x)) }{\\bm x} = \\pardiff{\\bm g} { \\bm x} \\pardiff{ f} { \\bm g} \\quad (p \\times q)(q \\times 1) \\] 더 나아가 다음도 보일 수 있다. \\[ \\pardiff{ \\bm f(\\bm g(\\bm u( \\bm x))) }{\\bm x} = \\pardiff{\\bm u} { \\bm x} \\pardiff{\\bm g} { \\bm u} \\pardiff{\\bm f} { \\bm g} \\] "],
["references.html", "References", " References Nelder, John Ashworth, and Robert WM Wedderburn. 1972. “Generalized Linear Models.” Journal of the Royal Statistical Society: Series A (General) 135 (3): 370–84. Searle, Shayle Robert, and Charles E McCulloch. 2001. Generalized, Linear and Mixed Models. Wiley. Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/. "]
]
